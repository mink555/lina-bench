#!/usr/bin/env python3
"""
Phase 4: ê²°ê³¼ ë¹„êµ & ì„±ëŠ¥ ê³¡ì„  ì‹œê°í™”

ì‚¬ìš©ë²•:
    python -m benchmark.compare_results                      # ìµœì‹  ê²°ê³¼ â†’ ë¦¬í¬íŠ¸ ìƒì„±
    python -m benchmark.compare_results --run-id 20260207    # íŠ¹ì • run_id í¬í•¨ ê²°ê³¼
    python -m benchmark.compare_results --charts             # ì°¨íŠ¸ë„ í•¨ê»˜ ìƒì„±
    python -m benchmark.compare_results --list               # ì €ì¥ëœ ê²°ê³¼ ëª©ë¡

ì‚°ì¶œë¬¼ (benchmark/results/ ì— ì €ì¥):
    report_{run_id}.txt                  â€” í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
    charts/ (--charts ì˜µì…˜ ì‹œ)           â€” ì„±ëŠ¥ ê³¡ì„  ì°¨íŠ¸ 7ì¢… (PNG)
"""

from __future__ import annotations

import argparse
import json
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path

import matplotlib
matplotlib.use("Agg")  # ì„œë²„/í„°ë¯¸ë„ í™˜ê²½
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import matplotlib.colors as mcolors
from matplotlib.patches import FancyBboxPatch
import numpy as np

# â”€â”€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path(__file__).resolve().parent.parent
RESULTS_DIR = ROOT / "benchmark" / "results"
CHARTS_DIR = RESULTS_DIR / "charts"

# â”€â”€ ì‹œë‚˜ë¦¬ì˜¤ ë¼ë²¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCENARIO_LABELS = {
    "O1_ST1": "ì²­ì•½/ì¡°ê±´ëˆ„ì ",
    "O1_ST2": "ì²­ì•½/ë§¥ë½í¬ì„",
    "O1_ST3": "ì²­ì•½/êµë€ì£¼ì…",
    "O2_ST1": "ë³´ë¥˜/ì¡°ê±´ëˆ„ì ",
    "O2_ST2": "ë³´ë¥˜/ë§¥ë½í¬ì„",
    "O2_ST3": "ë³´ë¥˜/êµë€ì£¼ì…",
}

TURN_CUTOFFS = [3, 5, 7, 10, 13, 15, 17, 19]

# ì‹¤ë¬´ êµ¬ê°„ cutoff â€” ì‹¤ì œ TMR ì½œì€ ëŒ€ë¶€ë¶„ 5~7í„´ ì´ë‚´
PRODUCTION_CUTOFF = 7

# êµ¬ê°„ íŒì • ì„ê³„ê°’ (85% = ì ˆëŒ€ ì„ê³„ì„ )
THRESHOLD_SAFE = 0.90
THRESHOLD_CRITICAL = 0.85   # â† ì ˆëŒ€ ë–¨ì–´ì§€ë©´ ì•ˆ ë˜ëŠ” í¬ì¸íŠ¸
THRESHOLD_WARNING = 0.75

# â”€â”€ ëª¨ë˜ ì°¨íŠ¸ ìŠ¤íƒ€ì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒ”ë ˆíŠ¸: êµ¬ë¶„ ëª…í™•í•œ 5ìƒ‰ (ìƒ‰ê°ì´ìƒ ì¹œí™”ì )
PALETTE = {
    "blue":    "#3B82F6",
    "red":     "#EF4444",
    "emerald": "#10B981",
    "violet":  "#8B5CF6",
    "amber":   "#F59E0B",
}
COLORS = list(PALETTE.values())
MARKERS = ["o", "D", "s", "^", "v"]

# ê¸€ë¡œë²Œ matplotlib ì„¤ì •
_RC = {
    "figure.facecolor":   "#FAFAFA",
    "axes.facecolor":     "#FFFFFF",
    "axes.edgecolor":     "#E5E7EB",
    "axes.grid":          True,
    "grid.color":         "#F3F4F6",
    "grid.linewidth":     0.8,
    "axes.spines.top":    False,
    "axes.spines.right":  False,
    "axes.labelsize":     11,
    "axes.titlesize":     14,
    "axes.titleweight":   "bold",
    "xtick.labelsize":    10,
    "ytick.labelsize":    10,
    "legend.fontsize":    9,
    "legend.framealpha":  0.9,
    "legend.edgecolor":   "#E5E7EB",
    "font.family":        "sans-serif",
    "font.sans-serif":    ["Helvetica Neue", "Arial", "DejaVu Sans"],
}
plt.rcParams.update(_RC)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Data Loading
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def find_latest_detail() -> Path | None:
    """results/ ì—ì„œ ê°€ì¥ ìµœê·¼ detail JSONì„ ì°¾ëŠ”ë‹¤."""
    files = sorted(RESULTS_DIR.glob("detail_*.json"), reverse=True)
    return files[0] if files else None


def find_detail_by_id(run_id: str) -> Path | None:
    """run_id ë¬¸ìì—´ì„ í¬í•¨í•˜ëŠ” detail JSONì„ ì°¾ëŠ”ë‹¤."""
    for f in sorted(RESULTS_DIR.glob("detail_*.json"), reverse=True):
        if run_id in f.name:
            return f
    return None


def list_results():
    """ì €ì¥ëœ ê²°ê³¼ ëª©ë¡ì„ ì¶œë ¥í•œë‹¤."""
    files = sorted(RESULTS_DIR.glob("detail_*.json"))
    if not files:
        print("  ì €ì¥ëœ ê²°ê³¼ ì—†ìŒ. ë¨¼ì € run_benchmark.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    print(f"\n  ì €ì¥ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ({len(files)}ê±´):")
    print(f"  {'â”€' * 70}")
    for f in files:
        with open(f) as fh:
            data = json.load(fh)
        meta = data.get("_meta", {})
        models = meta.get("models", [])
        elapsed = meta.get("elapsed_seconds", 0)
        total = meta.get("total_turns", 0)
        ts = meta.get("timestamp", "?")
        short_models = ", ".join(m.split("/")[-1][:20] for m in models[:3])
        if len(models) > 3:
            short_models += f" +{len(models)-3}"
        print(f"    {f.name}")
        print(f"      ì‹œê°„: {ts}  |  ì†Œìš”: {elapsed}s  |  í„´: {total}")
        print(f"      ëª¨ë¸: {short_models}")
        print()


def load_detail(path: Path) -> tuple[dict, dict]:
    """detail JSON â†’ (meta, results) ë°˜í™˜."""
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    meta = data.get("_meta", {})
    results = data.get("results", data)  # ì´ì „ í¬ë§· í˜¸í™˜
    if "_meta" in results:
        del results["_meta"]
    return meta, results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Metric Computation (from detail)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compute_turnpoint(
    results: dict,
    metric_path: str,
    *,
    exclude_no_call: bool = False,
) -> dict[str, dict[int, float]]:
    """
    ëª¨ë¸ë³„ë¡œ ê° cutoff ì§€ì ì˜ ëˆ„ì  metricì„ ê³„ì‚°.
    exclude_no_call=Trueì´ë©´ no_call í„´ì„ ì œì™¸ (BFCL ì§€í‘œìš©).
    Returns: {model: {cutoff: avg_value}}
    """
    output = {}
    for model, scenarios in results.items():
        # í„´ ë²ˆí˜¸ë³„ ìˆ˜ì§‘
        by_turn: dict[int, list[float]] = defaultdict(list)
        for sc_id, turns in scenarios.items():
            for t in turns:
                if exclude_no_call and t.get("call_type", "single") == "no_call":
                    continue
                parts = metric_path.split(".")
                val = t
                for p in parts:
                    val = val.get(p, {}) if isinstance(val, dict) else None
                    if val is None:
                        break
                if val is not None and isinstance(val, (int, float)):
                    by_turn[t["turn"]].append(float(val))

        # ê° cutoffì—ì„œ ëˆ„ì  í‰ê· 
        cutoff_vals = {}
        for c in TURN_CUTOFFS:
            vals = []
            for tn in range(1, c + 1):
                vals.extend(by_turn.get(tn, []))
            if vals:
                cutoff_vals[c] = sum(vals) / len(vals)
        output[model] = cutoff_vals

    return output


def compute_per_turn(
    results: dict,
    metric_path: str,
    *,
    exclude_no_call: bool = False,
) -> dict[str, dict[int, float]]:
    """ëª¨ë¸ë³„ë¡œ ê° ê°œë³„ í„´ì˜ metric í‰ê· .
    exclude_no_call=Trueì´ë©´ no_call í„´ ì œì™¸ (BFCL ì§€í‘œìš©)."""
    output = {}
    for model, scenarios in results.items():
        by_turn: dict[int, list[float]] = defaultdict(list)
        for sc_id, turns in scenarios.items():
            for t in turns:
                if exclude_no_call and t.get("call_type", "single") == "no_call":
                    continue
                parts = metric_path.split(".")
                val = t
                for p in parts:
                    val = val.get(p, {}) if isinstance(val, dict) else None
                    if val is None:
                        break
                if val is not None and isinstance(val, (int, float)):
                    by_turn[t["turn"]].append(float(val))

        output[model] = {tn: sum(v) / len(v) for tn, v in by_turn.items() if v}

    return output


def compute_single_parallel(results: dict) -> dict[str, dict]:
    """ëª¨ë¸ë³„ Single / Parallel / No-Call ë¶„ë¦¬ ì§‘ê³„."""
    output = {}
    for model, scenarios in results.items():
        single_tool = []
        single_arg = []
        par_tool = []
        par_arg = []
        par_detect = []
        nc_acc = []           # no_call ì •ë‹µ (tool ë¯¸í˜¸ì¶œ = 1)
        nc_slot_acc = []      # slot_question ì •ë‹µ
        nc_rel_acc = []       # relevance_detection ì •ë‹µ
        nc_nl_quality = []    # no_call í„´ì˜ NL Quality

        for sc_id, turns in scenarios.items():
            for t in turns:
                ct = t.get("call_type", "single")
                if ct == "no_call":
                    # NC:AccëŠ” FC Judgeì˜ action_type_acc ì‚¬ìš© (BFCL ëŒ€ìƒ ì•„ë‹˜)
                    nc_val = t["fc_judgment"]["action_type_acc"]
                    nc_acc.append(nc_val)
                    if t.get("gt_action") == "slot_question":
                        nc_slot_acc.append(nc_val)
                    elif t.get("gt_action") == "relevance_detection":
                        nc_rel_acc.append(nc_val)
                    # No-Call í„´ì˜ NL Quality ì§‘ê³„
                    if t.get("fc_quality") is not None:
                        nc_nl_quality.append(1.0 if t["fc_quality"].get("pass") else 0.0)
                elif ct == "single":
                    single_tool.append(t["bfcl"]["tool_name_acc"])
                    single_arg.append(t["bfcl"]["arg_value_acc"])
                else:
                    par_tool.append(t["bfcl"]["tool_name_acc"])
                    par_arg.append(t["bfcl"]["arg_value_acc"])
                    par_detect.append(t["bfcl"].get("parallel_detected", 0))

        # Slot Question SLOT-all completeness
        slot_complete_vals = []
        for sc_id, turns in scenarios.items():
            for t in turns:
                if t.get("gt_action") == "slot_question":
                    gt_cnt = t.get("gt_missing_count", 0)
                    asked = t.get("slot_asked_count")
                    if gt_cnt > 0 and asked is not None:
                        slot_complete_vals.append(min(asked / gt_cnt, 1.0))
        slot_completeness = sum(slot_complete_vals) / len(slot_complete_vals) if slot_complete_vals else 0

        output[model] = {
            "single_tool": sum(single_tool) / len(single_tool) if single_tool else 0,
            "single_arg": sum(single_arg) / len(single_arg) if single_arg else 0,
            "single_n": len(single_tool),
            "parallel_tool": sum(par_tool) / len(par_tool) if par_tool else 0,
            "parallel_arg": sum(par_arg) / len(par_arg) if par_arg else 0,
            "parallel_detect": sum(par_detect) / len(par_detect) if par_detect else 0,
            "parallel_n": len(par_tool),
            "nc_acc": sum(nc_acc) / len(nc_acc) if nc_acc else 0,
            "nc_slot_acc": sum(nc_slot_acc) / len(nc_slot_acc) if nc_slot_acc else 0,
            "nc_rel_acc": sum(nc_rel_acc) / len(nc_rel_acc) if nc_rel_acc else 0,
            "nc_slot_completeness": slot_completeness,
            "nc_nl_quality": sum(nc_nl_quality) / len(nc_nl_quality) if nc_nl_quality else None,
            "nc_n": len(nc_acc),
        }

    return output


def _turn_performance(t: dict) -> float:
    """í„´ í•˜ë‚˜ì˜ Performance ì¢…í•© ì ìˆ˜.

    tool_call í„´: (Tool + Arg + FC) / 3
    no_call í„´:   FC Judgeë§Œ (BFCLì€ 'í˜¸ì¶œí•´ì•¼ í•  í„´'ì—ì„œë§Œ ì¸¡ì •)
    """
    fcj_vals = list(t["fc_judgment"].values())
    fc = sum(fcj_vals) / len(fcj_vals) if fcj_vals else 0
    if t.get("call_type", "single") == "no_call":
        return fc
    tool = t["bfcl"]["tool_name_acc"]
    arg = t["bfcl"]["arg_value_acc"]
    return (tool + arg + fc) / 3


def compute_turnpoint_performance(
    results: dict,
) -> dict[str, dict[int, float]]:
    """ëª¨ë¸ë³„ ê° cutoff ì§€ì ì˜ ëˆ„ì  Performance ì¢…í•© ì ìˆ˜."""
    output = {}
    for model, scenarios in results.items():
        by_turn: dict[int, list[float]] = defaultdict(list)
        for sc_id, turns in scenarios.items():
            for t in turns:
                by_turn[t["turn"]].append(_turn_performance(t))

        cutoff_vals = {}
        for c in TURN_CUTOFFS:
            vals = []
            for tn in range(1, c + 1):
                vals.extend(by_turn.get(tn, []))
            if vals:
                cutoff_vals[c] = sum(vals) / len(vals)
        output[model] = cutoff_vals

    return output


def compute_turnpoint_performance_by_stress(
    results: dict,
) -> dict[str, dict[str, dict[int, float]]]:
    """ëª¨ë¸ë³„ Stress Type(ST1/ST2/ST3)ë³„ ê° cutoff ì§€ì ì˜ ëˆ„ì  Performance.

    Returns: {model: {"ST1": {cutoff: avg}, "ST2": ..., "ST3": ...}}
    """
    output = {}
    for model, scenarios in results.items():
        by_st_turn: dict[str, dict[int, list[float]]] = {
            st: defaultdict(list) for st in ("ST1", "ST2", "ST3")
        }
        for sc_id, turns in scenarios.items():
            st = sc_id.split("_")[1]  # "ST1", "ST2", "ST3"
            for t in turns:
                by_st_turn[st][t["turn"]].append(_turn_performance(t))

        model_result = {}
        for st in ("ST1", "ST2", "ST3"):
            cutoff_vals = {}
            for c in TURN_CUTOFFS:
                vals = []
                for tn in range(1, c + 1):
                    vals.extend(by_st_turn[st].get(tn, []))
                if vals:
                    cutoff_vals[c] = sum(vals) / len(vals)
            model_result[st] = cutoff_vals
        output[model] = model_result

    return output


def compute_turnpoint_fc(results: dict) -> dict[str, dict[int, float]]:
    """ëª¨ë¸ë³„ ê° cutoff ì§€ì ì˜ ëˆ„ì  FC Judgment í‰ê· ."""
    output = {}
    for model, scenarios in results.items():
        by_turn: dict[int, list[float]] = defaultdict(list)
        for sc_id, turns in scenarios.items():
            for t in turns:
                fcj_vals = list(t["fc_judgment"].values())
                if fcj_vals:
                    by_turn[t["turn"]].append(sum(fcj_vals) / len(fcj_vals))

        cutoff_vals = {}
        for c in TURN_CUTOFFS:
            vals = []
            for tn in range(1, c + 1):
                vals.extend(by_turn.get(tn, []))
            if vals:
                cutoff_vals[c] = sum(vals) / len(vals)
        output[model] = cutoff_vals

    return output


def compute_overall(results: dict) -> dict[str, dict]:
    """ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜ (Section 1ìš©).

    BFCL (Tool Acc, Arg Acc): tool_call í„´(single + parallel)ì—ì„œë§Œ ì‚°ì¶œ
    FC Judge: ì „ì²´ í„´(tool_call + no_call)ì—ì„œ ì‚°ì¶œ
    NL Quality: ì „ì²´ í„´ì—ì„œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° LLM-as-Judge í‰ê°€ (pass rate)
    Performance: í„´ë³„ _turn_performance()ì˜ í‰ê·  (turn-pointì™€ ë™ì¼ ë°©ì‹)
    """
    output = {}
    for model, scenarios in results.items():
        all_turns = [t for sc in scenarios.values() for t in sc]
        # BFCL: toolì„ í˜¸ì¶œí•´ì•¼ í•˜ëŠ” í„´ì—ì„œë§Œ
        tc_turns = [t for t in all_turns if t.get("call_type", "single") != "no_call"]
        tool = sum(t["bfcl"]["tool_name_acc"] for t in tc_turns) / len(tc_turns) if tc_turns else 0
        arg = sum(t["bfcl"]["arg_value_acc"] for t in tc_turns) / len(tc_turns) if tc_turns else 0
        # FC Judge: ì „ì²´ í„´ (no_call í¬í•¨)
        fcj_all = [v for t in all_turns for v in t["fc_judgment"].values()]
        fc = sum(fcj_all) / len(fcj_all) if fcj_all else 0
        # NL Quality: í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í„´ì—ì„œë§Œ (fc_qualityê°€ Noneì´ ì•„ë‹Œ ê²½ìš°)
        nl_evals = [t["fc_quality"] for t in all_turns if t.get("fc_quality") is not None]
        nl_pass = sum(1 for q in nl_evals if q.get("pass")) if nl_evals else 0
        nl_rate = nl_pass / len(nl_evals) if nl_evals else None
        # Performance: per-turn ë°©ì‹ (turn-point ê³„ì‚°ê³¼ ë™ì¼)
        # tool_call í„´: (Tool + Arg + FC) / 3
        # no_call í„´:   FC Judgeë§Œ
        perf_vals = [_turn_performance(t) for t in all_turns]
        perf = sum(perf_vals) / len(perf_vals) if perf_vals else 0
        output[model] = {
            "tool": tool, "arg": arg, "fc": fc, "nl_quality": nl_rate, "performance": perf,
            "tool_call_turns": len(tc_turns), "total_turns": len(all_turns),
        }
    return output


def compute_scenario_matrix(results: dict) -> dict[str, dict[str, float]]:
    """ëª¨ë¸ë³„ Ã— ì‹œë‚˜ë¦¬ì˜¤ë³„ tool_name_acc (tool_call í„´ì—ì„œë§Œ)."""
    output = {}
    for model, scenarios in results.items():
        sc_acc = {}
        for sc_id, turns in scenarios.items():
            tc_turns = [t for t in turns if t.get("call_type", "single") != "no_call"]
            vals = [t["bfcl"]["tool_name_acc"] for t in tc_turns]
            sc_acc[sc_id] = sum(vals) / len(vals) if vals else 0
        output[model] = sc_acc
    return output


def compute_stress_cross_analysis(results: dict) -> dict[str, dict]:
    """Stress Type Ã— Outcome êµì°¨ë¶„ì„ (Tool Acc + Performance)."""
    output = {}
    for model, scenarios in results.items():
        by_st_tool: dict[str, list[float]] = defaultdict(list)
        by_st_perf: dict[str, list[float]] = defaultdict(list)
        by_outcome_tool: dict[str, list[float]] = defaultdict(list)
        by_outcome_perf: dict[str, list[float]] = defaultdict(list)

        for sc_id, turns in scenarios.items():
            tc_turns = [t for t in turns if t.get("call_type", "single") != "no_call"]
            tool_vals = [t["bfcl"]["tool_name_acc"] for t in tc_turns]
            perf_vals = [_turn_performance(t) for t in turns]
            tool_avg = sum(tool_vals) / len(tool_vals) if tool_vals else 0
            perf_avg = sum(perf_vals) / len(perf_vals) if perf_vals else 0

            parts = sc_id.split("_")
            outcome = parts[0]  # "O1" or "O2"
            st = parts[1]       # "ST1", "ST2", "ST3"

            by_st_tool[st].append(tool_avg)
            by_st_perf[st].append(perf_avg)
            by_outcome_tool[outcome].append(tool_avg)
            by_outcome_perf[outcome].append(perf_avg)

        output[model] = {
            "st_tool": {k: sum(v) / len(v) for k, v in sorted(by_st_tool.items())},
            "st_perf": {k: sum(v) / len(v) for k, v in sorted(by_st_perf.items())},
            "outcome_tool": {k: sum(v) / len(v) for k, v in sorted(by_outcome_tool.items())},
            "outcome_perf": {k: sum(v) / len(v) for k, v in sorted(by_outcome_perf.items())},
        }
    return output


def find_threshold_turn(
    results: dict,
    metric_path: str,
    threshold: float,
) -> dict[str, int | None]:
    """ëª¨ë¸ë³„ë¡œ ëˆ„ì  metricì´ threshold ì•„ë˜ë¡œ ë–¨ì–´ì§€ëŠ” ì²« í„´."""
    output = {}
    for model, scenarios in results.items():
        by_turn: dict[int, list[float]] = defaultdict(list)
        for sc_id, turns in scenarios.items():
            for t in turns:
                parts = metric_path.split(".")
                val = t
                for p in parts:
                    val = val.get(p, {}) if isinstance(val, dict) else None
                    if val is None:
                        break
                if val is not None and isinstance(val, (int, float)):
                    by_turn[t["turn"]].append(float(val))

        max_t = max(by_turn.keys()) if by_turn else 0
        found = None
        for tn in range(1, max_t + 1):
            vals = []
            for n in range(1, tn + 1):
                vals.extend(by_turn.get(n, []))
            if vals and (sum(vals) / len(vals)) < threshold:
                found = tn
                break
        output[model] = found

    return output


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Chart Generation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _short(model: str) -> str:
    return model.split("/")[-1][:25]


def chart_turnpoint_curve(
    cumul: dict[str, dict[int, float]],
    title: str,
    ylabel: str,
    save_path: Path,
):
    """Turn-point ëˆ„ì  ì„±ëŠ¥ ê³¡ì„  â€” ëª¨ë˜ ë””ìì¸."""
    fig, ax = plt.subplots(figsize=(13, 6.5))

    # êµ¬ê°„ ë°°ê²½ (íŒŒìŠ¤í…”)
    ax.axvspan(2, 5.5,   alpha=0.06, color="#10B981", zorder=0)
    ax.axvspan(5.5, 10.5, alpha=0.06, color="#F59E0B", zorder=0)
    ax.axvspan(10.5, 20,  alpha=0.06, color="#EF4444", zorder=0)

    # êµ¬ê°„ ë¼ë²¨
    ax.text(3.75,  1.02, "Production", ha="center", fontsize=8,
            color="#059669", fontweight="bold", alpha=0.7)
    ax.text(8.0,   1.02, "Stress", ha="center", fontsize=8,
            color="#D97706", fontweight="bold", alpha=0.7)
    ax.text(15.25, 1.02, "Extreme", ha="center", fontsize=8,
            color="#DC2626", fontweight="bold", alpha=0.7)

    # ì„ê³„ì„ 
    for y, label, color in [
        (0.90, "SAFE  90%",       "#059669"),
        (0.85, "CRITICAL  85%",   "#DC2626"),
        (0.75, "WARNING  75%",    "#9CA3AF"),
    ]:
        ax.axhline(y=y, color=color, linestyle="--", linewidth=1, alpha=0.45, zorder=1)
        ax.text(max(TURN_CUTOFFS) + 0.5, y, label, va="center",
                fontsize=7.5, color=color, alpha=0.8, fontweight="bold")

    # ëª¨ë¸ ê³¡ì„ 
    for i, (model, vals) in enumerate(cumul.items()):
        xs = sorted(vals.keys())
        ys = [vals[x] for x in xs]
        c = COLORS[i % len(COLORS)]
        ax.plot(xs, ys, marker=MARKERS[i % len(MARKERS)],
                color=c, linewidth=2.5, markersize=9,
                markeredgecolor="white", markeredgewidth=1.5,
                label=_short(model), zorder=3)
        # ëì  ê°’ í‘œì‹œ
        if ys:
            ax.annotate(f"{ys[-1]:.0%}", (xs[-1], ys[-1]),
                        textcoords="offset points", xytext=(8, 0),
                        fontsize=8, color=c, fontweight="bold")

    ax.set_xlabel("Turn Cutoff")
    ax.set_ylabel(ylabel)
    ax.set_title(title, pad=18)
    ax.set_xticks(TURN_CUTOFFS)
    ax.set_xlim(2, max(TURN_CUTOFFS) + 2.5)
    ax.set_ylim(0, 1.08)
    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))
    ax.legend(loc="lower left", frameon=True, borderpad=0.8)

    fig.tight_layout()
    fig.savefig(save_path, dpi=180, bbox_inches="tight", facecolor=fig.get_facecolor())
    plt.close(fig)
    print(f"    chart: {save_path.name}")


def chart_stress_turnpoint(
    cumul_by_stress: dict[str, dict[str, dict[int, float]]],
    save_path: Path,
):
    """Stress Type(ST1/ST2/ST3)ë³„ Performance ê³¡ì„  â€” 3-subplot."""
    st_labels = {"ST1": "ST1 â€” State Accumulation",
                 "ST2": "ST2 â€” Context Drift",
                 "ST3": "ST3 â€” Distraction Injection"}
    models = list(cumul_by_stress.keys())
    fig, axes = plt.subplots(1, 3, figsize=(20, 6.5), sharey=True)

    for idx, st in enumerate(("ST1", "ST2", "ST3")):
        ax = axes[idx]

        # êµ¬ê°„ ë°°ê²½
        ax.axvspan(2, 5.5,   alpha=0.06, color="#10B981", zorder=0)
        ax.axvspan(5.5, 10.5, alpha=0.06, color="#F59E0B", zorder=0)
        ax.axvspan(10.5, 20,  alpha=0.06, color="#EF4444", zorder=0)

        # ì„ê³„ì„ 
        for y, color in [(0.90, "#059669"), (0.85, "#DC2626"), (0.75, "#9CA3AF")]:
            ax.axhline(y=y, color=color, linestyle="--", linewidth=0.8, alpha=0.4, zorder=1)

        # ëª¨ë¸ ê³¡ì„ 
        for i, model in enumerate(models):
            vals = cumul_by_stress[model].get(st, {})
            xs = sorted(vals.keys())
            ys = [vals[x] for x in xs]
            if not xs:
                continue
            c = COLORS[i % len(COLORS)]
            ax.plot(xs, ys, marker=MARKERS[i % len(MARKERS)],
                    color=c, linewidth=2.2, markersize=7,
                    markeredgecolor="white", markeredgewidth=1.2,
                    label=_short(model), zorder=3)
            if ys:
                ax.annotate(f"{ys[-1]:.0%}", (xs[-1], ys[-1]),
                            textcoords="offset points", xytext=(6, 0),
                            fontsize=7, color=c, fontweight="bold")

        ax.set_title(st_labels[st], fontsize=11, fontweight="bold", pad=10)
        ax.set_xlabel("Turn Cutoff")
        ax.set_xticks(TURN_CUTOFFS)
        ax.set_xlim(2, max(TURN_CUTOFFS) + 2.5)
        ax.set_ylim(0, 1.08)
        ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))
        if idx == 0:
            ax.set_ylabel("Cumulative Performance")
            ax.legend(loc="lower left", frameon=True, borderpad=0.6, fontsize=8)

    fig.suptitle("Performance by Stress Type", fontsize=14, fontweight="bold", y=1.02)
    fig.tight_layout()
    fig.savefig(save_path, dpi=180, bbox_inches="tight", facecolor=fig.get_facecolor())
    plt.close(fig)
    print(f"    chart: {save_path.name}")


def chart_per_turn(
    per_turn: dict[str, dict[int, float]],
    title: str,
    ylabel: str,
    save_path: Path,
):
    """ê°œë³„ í„´ ì •í™•ë„ ê³¡ì„  â€” ê¸‰ë½ ì§€ì  íƒì§€."""
    fig, ax = plt.subplots(figsize=(13, 6.5))

    # ìœ„í—˜ êµ¬ê°„
    ax.axhspan(0, 0.7, alpha=0.04, color="#EF4444", zorder=0)

    ax.axhline(y=0.9, color="#059669", linestyle="--", linewidth=0.8, alpha=0.4)
    ax.axhline(y=0.7, color="#DC2626", linestyle="--", linewidth=0.8, alpha=0.4)

    for i, (model, vals) in enumerate(per_turn.items()):
        xs = sorted(vals.keys())
        ys = [vals[x] for x in xs]
        c = COLORS[i % len(COLORS)]
        # ë©´ ì±„ìš°ê¸° (ì—°í•œ)
        ax.fill_between(xs, ys, alpha=0.06, color=c, zorder=1)
        ax.plot(xs, ys, marker=MARKERS[i % len(MARKERS)],
                color=c, linewidth=2, markersize=7,
                markeredgecolor="white", markeredgewidth=1.2,
                label=_short(model), zorder=3, alpha=0.9)

    ax.set_xlabel("Turn")
    ax.set_ylabel(ylabel)
    ax.set_title(title, pad=12)
    ax.set_ylim(-0.05, 1.08)
    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))
    ax.legend(loc="lower left", frameon=True, borderpad=0.8)

    fig.tight_layout()
    fig.savefig(save_path, dpi=180, bbox_inches="tight", facecolor=fig.get_facecolor())
    plt.close(fig)
    print(f"    chart: {save_path.name}")


def _bar_label(ax, bars, fmt="{:.0%}", offset=3, fontsize=8, color="#374151"):
    """ë°” ìœ„ì— ê°’ í‘œì‹œ (ê³µí†µ í—¬í¼)."""
    for bar in bars:
        h = bar.get_height()
        if h > 0.005:
            ax.text(bar.get_x() + bar.get_width() / 2, h + 0.015,
                    fmt.format(h), ha="center", va="bottom",
                    fontsize=fontsize, color=color, fontweight="medium")


def chart_single_vs_parallel(
    sp: dict[str, dict],
    save_path: Path,
):
    """Single vs Parallel â€” ê¹”ë”í•œ ê·¸ë£¹ ë°” ì°¨íŠ¸."""
    models = list(sp.keys())
    short_names = [_short(m) for m in models]
    n = len(models)
    x = np.arange(n)
    w = 0.32

    fig, axes = plt.subplots(1, 3, figsize=(16, 5.5),
                              gridspec_kw={"wspace": 0.30})

    c_s = PALETTE["blue"]
    c_p = PALETTE["amber"]
    c_d = PALETTE["emerald"]

    # --- (a) Tool Name Acc ---
    ax = axes[0]
    s_vals = [sp[m]["single_tool"] for m in models]
    p_vals = [sp[m]["parallel_tool"] for m in models]
    b1 = ax.bar(x - w / 2, s_vals, w, label="Single", color=c_s,
                edgecolor="white", linewidth=0.8, zorder=3)
    b2 = ax.bar(x + w / 2, p_vals, w, label="Parallel", color=c_p,
                edgecolor="white", linewidth=0.8, zorder=3)
    _bar_label(ax, b1, fontsize=7.5)
    _bar_label(ax, b2, fontsize=7.5)
    ax.set_ylabel("Tool Name Acc")
    ax.set_title("(a) Tool Name", fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(short_names, rotation=25, ha="right", fontsize=8.5)
    ax.set_ylim(0, 1.12)
    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))
    ax.legend(loc="upper right", fontsize=8)

    # --- (b) Arg Value Acc ---
    ax = axes[1]
    s_vals = [sp[m]["single_arg"] for m in models]
    p_vals = [sp[m]["parallel_arg"] for m in models]
    b1 = ax.bar(x - w / 2, s_vals, w, label="Single", color=c_s,
                edgecolor="white", linewidth=0.8, zorder=3)
    b2 = ax.bar(x + w / 2, p_vals, w, label="Parallel", color=c_p,
                edgecolor="white", linewidth=0.8, zorder=3)
    _bar_label(ax, b1, fontsize=7.5)
    _bar_label(ax, b2, fontsize=7.5)
    ax.set_ylabel("Arg Value Acc")
    ax.set_title("(b) Arg Value", fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(short_names, rotation=25, ha="right", fontsize=8.5)
    ax.set_ylim(0, 1.12)
    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))
    ax.legend(loc="upper right", fontsize=8)

    # --- (c) Parallel Detection ---
    ax = axes[2]
    d_vals = [sp[m]["parallel_detect"] for m in models]
    bars = ax.bar(x, d_vals, 0.48, color=c_d,
                  edgecolor="white", linewidth=0.8, zorder=3)
    _bar_label(ax, bars, fontsize=8.5, color="#065F46")
    ax.set_ylabel("Detection Rate")
    ax.set_title("(c) Parallel Detection", fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(short_names, rotation=25, ha="right", fontsize=8.5)
    ax.set_ylim(0, 1.12)
    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))

    fig.suptitle("Single vs Parallel Performance Comparison",
                 fontsize=15, fontweight="bold")
    fig.subplots_adjust(top=0.88, wspace=0.30)
    fig.savefig(save_path, dpi=180, bbox_inches="tight", facecolor=fig.get_facecolor())
    plt.close(fig)
    print(f"    chart: {save_path.name}")


def chart_scenario_heatmap(
    matrix: dict[str, dict[str, float]],
    save_path: Path,
):
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ â€” ì»¤ìŠ¤í…€ ì»¬ëŸ¬ë§µ."""
    models = list(matrix.keys())
    scenarios = sorted(
        set(sc for m in models for sc in matrix[m]),
        key=lambda x: (x[:2], x[3:]),
    )
    short_names = [_short(m) for m in models]

    CHART_LABELS = {
        "O1_ST1": "Sub / Accum",
        "O1_ST2": "Sub / Drift",
        "O1_ST3": "Sub / Distract",
        "O2_ST1": "Hold / Accum",
        "O2_ST2": "Hold / Drift",
        "O2_ST3": "Hold / Distract",
    }
    sc_labels = [f"{s}\n{CHART_LABELS.get(s, '')}" for s in scenarios]

    data = np.array([
        [matrix[m].get(s, 0) for s in scenarios]
        for m in models
    ])

    # ì»¤ìŠ¤í…€ ì»¬ëŸ¬ë§µ: ë¹¨ê°• â†’ ë…¸ë‘ â†’ ì´ˆë¡ (ë” ë¶€ë“œëŸ¬ìš´ í†¤)
    cmap = mcolors.LinearSegmentedColormap.from_list(
        "perf",
        ["#FCA5A5", "#FDE68A", "#6EE7B7", "#059669"],
        N=256,
    )

    fig, ax = plt.subplots(
        figsize=(max(10, len(scenarios) * 1.6), max(4.5, len(models) * 1.0 + 1)),
    )

    im = ax.imshow(data, cmap=cmap, vmin=0, vmax=1, aspect="auto")

    # ê²©ìì„ 
    for i in range(len(models) + 1):
        ax.axhline(y=i - 0.5, color="white", linewidth=2)
    for j in range(len(scenarios) + 1):
        ax.axvline(x=j - 0.5, color="white", linewidth=2)

    ax.set_xticks(range(len(scenarios)))
    ax.set_xticklabels(sc_labels, fontsize=9.5)
    ax.set_yticks(range(len(models)))
    ax.set_yticklabels(short_names, fontsize=10)

    # ì…€ ê°’ + í…Œë‘ë¦¬ íš¨ê³¼
    for i in range(len(models)):
        for j in range(len(scenarios)):
            val = data[i, j]
            txt_color = "#1F2937" if val > 0.45 else "#FAFAFA"
            weight = "bold" if val >= 0.9 or val <= 0.1 else "medium"
            ax.text(j, i, f"{val:.0%}", ha="center", va="center",
                    color=txt_color, fontsize=12, fontweight=weight)

    ax.set_title("Tool Name Accuracy by Scenario", pad=14)
    cb = fig.colorbar(im, ax=ax, format=mticker.PercentFormatter(1.0),
                      shrink=0.8, pad=0.02)
    cb.outline.set_visible(False)

    fig.tight_layout()
    fig.savefig(save_path, dpi=180, bbox_inches="tight", facecolor=fig.get_facecolor())
    plt.close(fig)
    print(f"    chart: {save_path.name}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Text Report Generation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Error Taxonomy
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 6ê°œ ì—ëŸ¬ íƒœê·¸ ì •ì˜
ERROR_TAGS = {
    "WRONG_TOOL":  "í˜¸ì¶œí•´ì•¼ í•˜ëŠ”ë° ë‹¤ë¥¸ tool í˜¸ì¶œ",
    "MISSED_CALL": "í˜¸ì¶œí•´ì•¼ í•˜ëŠ”ë° í˜¸ì¶œ ì•ˆ í•¨",
    "FALSE_CALL":  "í˜¸ì¶œí•˜ë©´ ì•ˆ ë˜ëŠ”ë° í˜¸ì¶œ",
    "ARG_MISSING": "tool ë§ì§€ë§Œ í•„ìˆ˜ ì¸ì ëˆ„ë½",
    "ARG_WRONG":   "tool ë§ì§€ë§Œ ì¸ì ê°’ í‹€ë¦¼",
    "ARG_STALE":   "ë²ˆë³µê°’ ë¯¸ê°±ì‹  (ST3 ì¶”ì •)",
}


def compute_error_taxonomy(results: dict) -> dict[str, dict[str, int]]:
    """ëª¨ë¸ë³„ ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜.

    Returns: {model: {tag: count, ..., "_total": N, "_correct": N}}
    """
    output = {}
    for model, scenarios in results.items():
        counts = {tag: 0 for tag in ERROR_TAGS}
        total = 0
        correct = 0

        for sc_id, turns in scenarios.items():
            is_st3 = "ST3" in sc_id
            for t in turns:
                total += 1
                ct = t.get("call_type", "single")
                has_calls = bool(t.get("model_tools")) or (
                    t["bfcl"]["tool_name_acc"] > 0 or
                    t["fc_judgment"]["action_type_acc"] == 1.0
                )

                # model_toolsê°€ ì—†ëŠ” ê²½ìš° fc_judgmentë¡œ ì¶”ë¡ 
                if "model_tools" in t:
                    has_calls = bool(t["model_tools"])
                else:
                    # tool_call í„´: action_type_acc=1 â†’ í˜¸ì¶œí•¨
                    # no_call í„´: action_type_acc=0 â†’ í˜¸ì¶œí•¨ (ì˜¤ë‹µ)
                    if ct == "no_call":
                        has_calls = t["fc_judgment"]["action_type_acc"] == 0.0
                    else:
                        has_calls = t["fc_judgment"]["action_type_acc"] == 1.0

                if ct == "no_call":
                    # ë¯¸í˜¸ì¶œì´ ì •ë‹µ
                    if has_calls:
                        counts["FALSE_CALL"] += 1
                    else:
                        correct += 1
                else:
                    # ì½œì´ ì •ë‹µ
                    tool_ok = t["bfcl"]["tool_name_acc"] == 1.0
                    arg_key_ok = t["bfcl"]["arg_key_acc"] == 1.0
                    arg_val_ok = t["bfcl"]["arg_value_acc"] == 1.0

                    if not has_calls:
                        counts["MISSED_CALL"] += 1
                    elif not tool_ok:
                        counts["WRONG_TOOL"] += 1
                    elif not arg_key_ok:
                        counts["ARG_MISSING"] += 1
                    elif not arg_val_ok:
                        if is_st3:
                            counts["ARG_STALE"] += 1
                        else:
                            counts["ARG_WRONG"] += 1
                    else:
                        correct += 1

        counts["_total"] = total
        counts["_correct"] = correct
        output[model] = counts
    return output


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Git / Config Helpers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _get_git_rev() -> str:
    """í˜„ì¬ git commit hash (short). ì‹¤íŒ¨ ì‹œ 'unknown'."""
    import subprocess
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--short", "HEAD"],
            capture_output=True, text=True, timeout=5,
            cwd=str(ROOT),
        )
        return result.stdout.strip() if result.returncode == 0 else "unknown"
    except Exception:
        return "unknown"


def _format_config(meta: dict) -> list[str]:
    """ë©”íƒ€ë°ì´í„°ì—ì„œ config ì •ë³´ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ."""
    cfg = meta.get("config", {})
    gen = cfg.get("generation", {})
    jdg = cfg.get("judge", {})
    lines = []
    lines.append(f"  seed={gen.get('seed', '?')}  "
                 f"temp={gen.get('temperature', '?')}  "
                 f"tool_choice={gen.get('tool_choice', '?')}")
    lines.append(f"  judge: seed={jdg.get('seed', '?')}  "
                 f"temp={jdg.get('temperature', '?')}  "
                 f"max_tokens={jdg.get('max_tokens', '?')}")
    return lines


def _zone(val: float) -> str:
    if val >= THRESHOLD_SAFE:
        return "SAFE"        # 90%+ : ì•ˆì •
    if val >= THRESHOLD_CRITICAL:
        return "GOOD"        # 85~90%: ì–‘í˜¸
    if val >= THRESHOLD_WARNING:
        return "RISK"        # 75~85%: 85% ë¯¸ë§Œ â€” ìœ„í—˜
    return "DANGER"          # 75% ë¯¸ë§Œ: ì‚¬ìš© ë¶ˆê°€


def _zone_dot(val: float) -> str:
    """ìˆ«ì% + ìƒ‰ìƒ ì›í˜• ì´ëª¨ì§€.
    í‘œì‹œ ê°’(ë°˜ì˜¬ë¦¼)ê³¼ ì´ëª¨ì§€ê°€ ë¶ˆì¼ì¹˜í•˜ì§€ ì•Šë„ë¡ ë°˜ì˜¬ë¦¼ í›„ íŒì •."""
    pct = round(val * 100)
    if pct >= 90:
        return f"{val:>5.0%} ğŸŸ¢"
    if pct >= 85:
        return f"{val:>5.0%} ğŸ”µ"
    if pct >= 75:
        return f"{val:>5.0%} ğŸŸ¡"
    return f"{val:>5.0%} ğŸ”´"


def _turnpoint_table(w, label: str, cumul: dict, models: list, as_zone=False):
    """Turn-point í…Œì´ë¸” í—¬í¼ (ê°’ ë˜ëŠ” êµ¬ê°„ë¼ë²¨)."""
    col_w = 9 if as_zone else 7
    hdr = f"    {'ëª¨ë¸':<28}"
    for c in TURN_CUTOFFS:
        hdr += f" {'~T' + str(c):>{col_w}}"
    w(hdr)
    sep_unit = "â”€" * (col_w + 1)
    w(f"    {'â”€' * 28}" + sep_unit * len(TURN_CUTOFFS))
    for model in models:
        row = f"    {_short(model):<28}"
        for c in TURN_CUTOFFS:
            v = cumul[model].get(c)
            if v is None:
                row += f" {'  -  ':>{col_w}}"
            elif as_zone:
                row += f" {_zone_dot(v)}"
            else:
                row += f" {v:>6.0%} "
        w(row)


def generate_report(
    meta: dict,
    results: dict,
    save_path: Path,
):
    """í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸."""
    lines: list[str] = []
    w = lines.append

    models = list(results.keys())
    overall = compute_overall(results)
    sp = compute_single_parallel(results)
    cumul_tool = compute_turnpoint(results, "bfcl.tool_name_acc", exclude_no_call=True)
    cumul_arg = compute_turnpoint(results, "bfcl.arg_value_acc", exclude_no_call=True)
    cumul_fc = compute_turnpoint_fc(results)
    cumul_perf = compute_turnpoint_performance(results)
    cumul_perf_st = compute_turnpoint_performance_by_stress(results)
    cross = compute_stress_cross_analysis(results)

    # ì‹¤ë¬´ êµ¬ê°„ Performance (PRODUCTION_CUTOFF ê¸°ì¤€)
    prod_perf: dict[str, float] = {}
    for model in models:
        prod_perf[model] = cumul_perf.get(model, {}).get(PRODUCTION_CUTOFF, 0)

    # Pre-compute safe turns (85% ì„ê³„ì„  ê¸°ë°˜)
    safe_turns: dict[str, int] = {}
    for model in models:
        _vals = cumul_perf[model]
        _d85 = None
        for _c in TURN_CUTOFFS:
            _v = _vals.get(_c)
            if _v is not None and _d85 is None and _v < THRESHOLD_CRITICAL:
                _d85 = _c
        if _d85:
            _idx = TURN_CUTOFFS.index(_d85)
            safe_turns[model] = TURN_CUTOFFS[_idx - 1] if _idx > 0 else 0
        else:
            safe_turns[model] = max(TURN_CUTOFFS)

    best_perf_raw = max(models, key=lambda m: overall[m]["performance"])
    best_par_model = max(models, key=lambda m: sp[m]["parallel_tool"])
    # NL Quality 1ìœ„ (N/A ì œì™¸)
    nl_candidates = [m for m in models if overall[m].get("nl_quality") is not None
                     and overall[m]["performance"] >= 0.30]
    best_nl_model = max(nl_candidates, key=lambda m: overall[m]["nl_quality"]) if nl_candidates else None

    # Agent 1ìœ„: Perfê°€ 3%p ì´ë‚´ë©´ safe_turns â†’ NL ìˆœìœ¼ë¡œ ì¢…í•© íŒë‹¨
    # (ë‹¨ìˆœ Perf í‰ê· ë³´ë‹¤ "85%+ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í•˜ëŠëƒ"ê°€ ì‹¤ë¬´ì—ì„œ ë” ì¤‘ìš”)
    top_perf = overall[best_perf_raw]["performance"]
    contenders = [m for m in models
                  if overall[m]["performance"] >= top_perf - 0.03
                  and overall[m]["performance"] >= 0.30]
    best_perf_model = max(
        contenders,
        key=lambda m: (
            safe_turns.get(m, 0),           # 1st: 85%+ ìœ ì§€ í„´ ìˆ˜
            overall[m]["performance"],       # 2nd: Perf í‰ê· 
            overall[m].get("nl_quality") or 0,  # 3rd: NL Quality
        ),
    )

    # â”€â”€ Pre-compute helper data â”€â”€
    common_safe_t = min(safe_turns.values()) if safe_turns else 0
    sorted_perf = sorted(models, key=lambda m: overall[m]["performance"], reverse=True)
    unusable = [m for m in models if overall[m]["performance"] < 0.30]
    usable = [m for m in sorted_perf if overall[m]["performance"] >= 0.30]
    best_nc_model = max(models, key=lambda m: sp[m]["nc_acc"])

    # Cross-analysis helpers
    valid_models = [m for m in models if len(cross[m]["st_tool"]) >= 3]
    st1_worst_cnt = sum(
        1 for m in valid_models
        if cross[m]["st_perf"].get("ST1", 1) <= min(cross[m]["st_perf"].values())
    )
    outcome_diffs = [
        abs(cross[m]["outcome_tool"].get("O1", 0) - cross[m]["outcome_tool"].get("O2", 0))
        for m in models
    ]
    avg_outcome_diff = sum(outcome_diffs) / len(outcome_diffs) if outcome_diffs else 0

    # ST ìˆœì„œ (average performance across usable models)
    st_avg = {}
    for st in ["ST1", "ST2", "ST3"]:
        vals_st = [cross[m]["st_perf"].get(st, 0) for m in usable]
        st_avg[st] = sum(vals_st) / len(vals_st) if vals_st else 0
    st_order = sorted(st_avg, key=st_avg.get, reverse=True)
    st_names = {"ST1": "ì¡°ê±´ëˆ„ì ", "ST2": "ë§¥ë½í¬ì„", "ST3": "êµë€ì£¼ì…"}

    # â”€â”€ Error Taxonomy ì‚¬ì „ ê³„ì‚° â”€â”€
    err_tax = compute_error_taxonomy(results)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HEADER
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  AI TMR Assistant â€” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
    w("=" * 78)
    w(f"  ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M')} | "
      f"í„´: {meta.get('total_turns', '?')} Ã— {len(models)}ëª¨ë¸ | "
      f"Judge: {meta.get('judge_model', 'N/A')}")
    git_rev = _get_git_rev()
    w(f"  commit: {git_rev} | run_id: {meta.get('run_id', '?')}")
    for cfg_line in _format_config(meta):
        w(f"  config: {cfg_line}")
    w("")
    w(f"  ìš©ì–´ ì •ì˜:")
    w(f"    @T7 (ì‹¤ë¬´ êµ¬ê°„) = Turn 1~7ê¹Œì§€ì˜ ëˆ„ì  ì„±ëŠ¥. TMR ì˜ì—…ì½œì˜")
    w(f"    ì‹¤ë¬´ í„´ ìˆ˜(ì²­ì•½ ~7í„´, ë³´ë¥˜ ~5í„´)ì— ëŒ€ì‘í•˜ëŠ” ìš´ì˜ ê¸°ì¤€ì„ .")
    w(f"    T7 ì´í›„(T10~T19)ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ êµ¬ê°„ìœ¼ë¡œ ë‚´êµ¬ë„ ì§„ë‹¨ìš©.")
    w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # â˜… ìš”ì•½
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    best_m = _short(best_perf_model)
    best_p = overall[best_perf_model]["performance"]
    best_safe = safe_turns[best_perf_model]
    best_safe_str = f"~T{best_safe}" if best_safe > 0 else "T3 ë¯¸ë§Œ"
    worst_st = st_order[-1] if st_order else "ST1"

    # Agent/NL ëª¨ë¸ ê¶Œì¥ íŒë‹¨
    best_nl_short = _short(best_nl_model) if best_nl_model else "N/A"
    best_nl_rate = overall[best_nl_model]["nl_quality"] if best_nl_model else 0
    same_model = best_perf_model == best_nl_model
    if same_model:
        model_strategy = f"1ëª¨ë¸ ê¶Œì¥: {best_m} (Agent+ë‹µë³€ ê²¸ìš©)"
    elif best_nl_model:
        model_strategy = f"Agent: {best_m} | ë‹µë³€: {best_nl_short} (2ëª¨ë¸ ë¶„ë¦¬ ê³ ë ¤)"
    else:
        model_strategy = f"Agent: {best_m} (NL ë°ì´í„° ë¶€ì¡±)"

    best_prod = prod_perf[best_perf_model]

    # 1ìœ„ ëª¨ë¸ì˜ ì‹¤ë¬´ êµ¬ê°„ ì„¸ë¶€ ì§€í‘œ ê³„ì‚° (ë³‘ëª© ë¶„ì„ìš©)
    _bp_tc_tool, _bp_tc_arg, _bp_tc_fc, _bp_nc_fc = [], [], [], []
    for _sc_id, _turns in results[best_perf_model].items():
        for _t in _turns:
            if _t["turn"] > PRODUCTION_CUTOFF:
                continue
            _fcj = _t["fc_judgment"]
            _fc_avg = sum(_fcj.values()) / len(_fcj.values()) if _fcj else 0
            if _t.get("call_type", "single") == "no_call":
                _bp_nc_fc.append(_fc_avg)
            else:
                _bp_tc_tool.append(_t["bfcl"]["tool_name_acc"])
                _bp_tc_arg.append(_t["bfcl"]["arg_value_acc"])
                _bp_tc_fc.append(_fc_avg)
    _bp_prod_tool = sum(_bp_tc_tool) / len(_bp_tc_tool) if _bp_tc_tool else 0
    _bp_prod_arg = sum(_bp_tc_arg) / len(_bp_tc_arg) if _bp_tc_arg else 0
    _bp_prod_fc = sum(_bp_tc_fc) / len(_bp_tc_fc) if _bp_tc_fc else 0
    _bp_prod_nc = sum(_bp_nc_fc) / len(_bp_nc_fc) if _bp_nc_fc else 0
    _bp_tc_n = len(_bp_tc_tool)
    _bp_nc_n = len(_bp_nc_fc)
    _bp_total_n = _bp_tc_n + _bp_nc_n

    # ë³‘ëª© ì‹ë³„
    _bottlenecks = []
    if _bp_prod_arg < 0.85:
        _bottlenecks.append(("Arg Acc", _bp_prod_arg))
    if _bp_prod_nc < 0.50:
        _bottlenecks.append(("No-Call", _bp_prod_nc))
    _bottleneck_str = " / ".join(f"{n} {v:.0%}" for n, v in _bottlenecks)

    w("=" * 78)
    w(f"  ì‹¤ë¬´ ê¸°ì¤€({PRODUCTION_CUTOFF}í„´) 1ìœ„: {best_m}  Performance {best_prod:.0%}")
    w(f"  ë³‘ëª©: {_bottleneck_str}  |  NL 1ìœ„: {best_nl_short} ({best_nl_rate:.0%})")
    w(f"  â†’ {model_strategy}")
    w("=" * 78)
    w(f"  â€» ë³¸ ë²¤ì¹˜ë§ˆí¬ëŠ” ìµœëŒ€ 19í„´ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.")
    w(f"    ì‹¤ë¬´ TMR ì½œì€ ë³´í†µ 5~7í„´ì´ë¯€ë¡œ, @T{PRODUCTION_CUTOFF} ëˆ„ì ì„ ì‹¤ë¬´ ì„±ëŠ¥ìœ¼ë¡œ ë´…ë‹ˆë‹¤.")
    w(f"    T{PRODUCTION_CUTOFF} ì´í›„ëŠ” ë‚´êµ¬ë„ ì§„ë‹¨ìš©ì´ë©°, ìš´ì˜ ëª©í‘œ ìˆ˜ì¹˜ê°€ ì•„ë‹™ë‹ˆë‹¤.")
    w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. í•µì‹¬ ì„±ì í‘œ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  1. ëª¨ë¸ë³„ ì„±ì í‘œ")
    w("=" * 78)
    w("")
    total_t = meta.get("total_turns", 106)
    tc_example = list(overall.values())[0].get("tool_call_turns", 94) if overall else 94
    w(f"  Tool Acc = tool í˜¸ì¶œ ì •ë‹µë¥  ({tc_example}í„´)")
    w(f"  Arg Acc  = ì¸ì ì •í™•ë„ (tool name ì •ë‹µì¼ ë•Œë§Œ)")
    w(f"  FC Judge = í–‰ë™ íŒë‹¨ ì •í™•ë„ (ì „ì²´ {total_t}í„´)")
    w(f"  NL Qual  = ìì—°ì–´ ë‹µë³€ í’ˆì§ˆ (LLM Judge, í…ìŠ¤íŠ¸ ìˆëŠ” í„´ë§Œ)")
    w(f"  Perf     = ì¢…í•© (toolí„´: (Tool+Arg+FC)/3, no-callí„´: FC)")
    w(f"  â€» ì‹¤ë¬´ = ~T{PRODUCTION_CUTOFF} ëˆ„ì  | ì „ì²´ = ~T{max(TURN_CUTOFFS)} ëˆ„ì  (ìŠ¤íŠ¸ë ˆìŠ¤ í¬í•¨)")
    w("")
    w(f"  {'ëª¨ë¸':<28} {'Tool':>7} {'Arg':>7} {'FC':>7} {'NL':>7}"
      f" {'â”‚ ì‹¤ë¬´':>7} {'ì „ì²´':>6} {'Gap':>6}")
    w(f"  {'â”€' * 28} {'â”€' * 7} {'â”€' * 7} {'â”€' * 7} {'â”€' * 7}"
      f" {'â”€' * 7} {'â”€' * 6} {'â”€' * 6}")
    for model in models:
        o = overall[model]
        pp = prod_perf[model]
        fp = o["performance"]
        gap = pp - fp
        marks = []
        if model in unusable:
            marks.append("âœ—")
        else:
            if model == best_perf_model:
                marks.append("Agent1ìœ„")
            if model == best_nl_model:
                marks.append("NL1ìœ„")
        mark_str = f" â—€ {','.join(marks)}" if marks else ""
        nl_str = f"{o['nl_quality']:>6.0%}" if o["nl_quality"] is not None else "  N/A "
        w(f"  {_short(model):<28}"
          f" {o['tool']:>6.1%}"
          f" {o['arg']:>6.1%}"
          f" {o['fc']:>6.1%}"
          f" {nl_str}"
          f" â”‚{pp:>5.0%}"
          f" {fp:>5.0%}"
          f" {gap:>+5.0%}p{mark_str}")

    w("")
    # Agent 1ìœ„ ì„ ì • ê·¼ê±°
    if best_perf_model != best_perf_raw:
        raw_s = _short(best_perf_raw)
        raw_pp = prod_perf[best_perf_raw]
        raw_fp = overall[best_perf_raw]["performance"]
        agent_s = _short(best_perf_model)
        agent_pp = prod_perf[best_perf_model]
        agent_fp = overall[best_perf_model]["performance"]
        agent_safe = safe_turns[best_perf_model]
        agent_safe_str = f"~T{agent_safe}" if agent_safe > 0 else "T3 ë¯¸ë§Œ"
        w(f"  â€» ì „ì²´ Perf 1ìœ„ëŠ” {raw_s}(ì‹¤ë¬´ {raw_pp:.0%} â†’ ì „ì²´ {raw_fp:.0%})ì´ë‚˜,")
        w(f"    {agent_s}(ì‹¤ë¬´ {agent_pp:.0%} â†’ ì „ì²´ {agent_fp:.0%})ê°€ "
          f"85%+ {agent_safe_str}ê¹Œì§€ ìœ ì§€ â†’ Agent 1ìœ„.")
    w("")

    # â”€â”€ 1ìœ„ ëª¨ë¸ ì‹¤ë¬´ êµ¬ê°„ ë³‘ëª© â”€â”€
    w(f"  [1ìœ„ ëª¨ë¸ ì‹¤ë¬´ êµ¬ê°„(@T{PRODUCTION_CUTOFF}) ì„¸ë¶€]")
    w(f"    ëŒ€ìƒ: {best_m} | ì‹¤ë¬´ í„´: {_bp_total_n}í„´ "
      f"(tool_call {_bp_tc_n} + no_call {_bp_nc_n})")
    def _bp_label(v, hi=0.85, lo=0.75):
        if v >= hi: return "ì´ë¯¸ ìš°ìˆ˜"
        if v >= lo: return "ì–‘í˜¸"
        return "ğŸ”´ ë³‘ëª©"
    w(f"    Tool Acc  {_bp_prod_tool:>5.0%}  â† {_bp_label(_bp_prod_tool)}")
    w(f"    Arg Acc   {_bp_prod_arg:>5.0%}  â† {_bp_label(_bp_prod_arg)}")
    w(f"    FC Judge  {_bp_prod_fc:>5.0%}  â† {_bp_label(_bp_prod_fc)}")
    w(f"    No-Call   {_bp_prod_nc:>5.0%}  â† {_bp_label(_bp_prod_nc, hi=0.70, lo=0.50)}")
    w(f"    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    w(f"    Perf      {best_prod:>5.0%}")
    w(f"    â†’ ê°œì„  ìš°ì„ ìˆœìœ„: "
      + " > ".join(f"{n}({v:.0%})" for n, v in
                   sorted(_bottlenecks, key=lambda x: x[1])))
    w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. ëŠ¥ë ¥ í•´ë¶€ â€” 3ê°œ í…Œì´ë¸”
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  2. ëŠ¥ë ¥ í•´ë¶€ â€” Single / Parallel / No-Call")
    w("=" * 78)
    n_s = sp[models[0]]["single_n"] if models else 82
    n_p = sp[models[0]]["parallel_n"] if models else 12
    n_nc = sp[models[0]]["nc_n"] if models else 12
    w("")

    # â”€â”€ 2a. Single â”€â”€
    w(f"  [Single â€” tool 1ê°œ í˜¸ì¶œ ({n_s}í„´)]")
    w(f"    {'ëª¨ë¸':<28} {'tool ì •ë‹µ':>9} {'ì¸ì ì •ë‹µ':>10}")
    w(f"    {'â”€' * 28} {'â”€' * 9} {'â”€' * 10}")
    for model in models:
        d = sp[model]
        w(f"    {_short(model):<28} {d['single_tool']:>8.0%} {d['single_arg']:>9.0%}")
    w("")

    # â”€â”€ 2b. Parallel â”€â”€
    w(f"  [Parallel â€” tool 2ê°œ ë™ì‹œ í˜¸ì¶œ ({n_p}í„´)]")
    w(f"    {'ëª¨ë¸':<28} {'tool ì •ë‹µ':>9} {'ì¸ì ì •ë‹µ':>10} {'2ê°œ ì¸ì‹':>9}")
    w(f"    {'â”€' * 28} {'â”€' * 9} {'â”€' * 10} {'â”€' * 9}")
    for model in models:
        d = sp[model]
        w(f"    {_short(model):<28} {d['parallel_tool']:>8.0%} {d['parallel_arg']:>9.0%} {d['parallel_detect']:>8.0%}")
    w(f"    â†’ ìµœê³  {sp[best_par_model]['parallel_detect']:.0%}. ì‹¤ì„œë¹„ìŠ¤ì—ì„œëŠ” 1ê°œì”© ë¶„ë¦¬ í˜¸ì¶œ í•„ìš”.")
    w("")

    # â”€â”€ 2c. No-Call â”€â”€
    w(f"  [No-Call â€” tool ì•ˆ ë¶ˆëŸ¬ì•¼ ì •ë‹µ ({n_nc}í„´)]")
    w(f"    {'ëª¨ë¸':<28} {'ë¯¸í˜¸ì¶œ ì •ë‹µ':>11} {'ì§ˆë¬¸':>7} {'ê±°ë¶€':>7} {'ëˆ„ë½ ì „ë¶€ ì§ˆë¬¸':>14} {'í…ìŠ¤íŠ¸ í’ˆì§ˆ':>12}")
    w(f"    {'â”€' * 28} {'â”€' * 11} {'â”€' * 7} {'â”€' * 7} {'â”€' * 14} {'â”€' * 12}")
    for model in models:
        d = sp[model]
        nl_str = f"{d['nc_nl_quality']:>11.0%}" if d.get("nc_nl_quality") is not None else "        N/A"
        w(f"    {_short(model):<28} {d['nc_acc']:>10.0%} {d['nc_slot_acc']:>6.0%} {d['nc_rel_acc']:>6.0%} {d['nc_slot_completeness']:>13.0%} {nl_str}")

    nc_perfect_fake = [m for m in models
                       if sp[m]["nc_acc"] >= 0.99 and overall[m]["tool"] < 0.50]
    nc_fail = [m for m in models
               if sp[m]["nc_acc"] < 0.50 and overall[m]["performance"] > 0.30]
    if nc_perfect_fake:
        w(f"    âš  {', '.join(_short(m) for m in nc_perfect_fake)}: "
          f"100%ì´ì§€ë§Œ tool ìì²´ë¥¼ ëª» ë¶ˆëŸ¬ì„œ ë†’ì€ ê²ƒ (ì˜ë¯¸ ì—†ìŒ)")
    if nc_fail:
        w(f"    âš  {', '.join(_short(m) for m in nc_fail)}: "
          f"ì •ë³´ ë¶€ì¡±í•´ë„ tool í˜¸ì¶œ â†’ ìœ„í—˜")
    w("")

    # â”€â”€ Tool í˜¸ì¶œ ì„±í–¥ ë¶„ì„ (trade-off) â”€â”€
    # Tool Acc ë†’ì§€ë§Œ No-Call ë‚®ì€ ëª¨ë¸ vs ê·¸ ë°˜ëŒ€
    aggressive = [m for m in usable if overall[m]["tool"] >= 0.70 and sp[m]["nc_acc"] < 0.50]
    conservative = [m for m in usable if sp[m]["nc_acc"] >= 0.80 and overall[m]["tool"] < 0.60]
    if aggressive or conservative:
        w(f"  [No-Call vs Tool í˜¸ì¶œ â€” trade-off ë¶„ì„]")
        w(f"    {'ëª¨ë¸':<28} {'Tool Acc':>9} {'NC ì •ë‹µ':>8} {'ì„±í–¥':>14}")
        w(f"    {'â”€' * 28} {'â”€' * 9} {'â”€' * 8} {'â”€' * 14}")
        for model in models:
            if model in unusable:
                continue
            t_acc = overall[model]["tool"]
            nc = sp[model]["nc_acc"]
            if t_acc >= 0.70 and nc < 0.50:
                tendency = "tool ê³¼ì‰"
            elif nc >= 0.80 and t_acc < 0.60:
                tendency = "tool ë¶€ì¡±"
            elif t_acc >= 0.70 and nc >= 0.60:
                tendency = "ê· í˜•"
            else:
                tendency = "-"
            w(f"    {_short(model):<28} {t_acc:>8.0%} {nc:>7.0%} {tendency:>14}")
        if aggressive:
            w(f"    â†’ tool ê³¼ì‰ ({len(aggressive)}ê°œ ëª¨ë¸): "
              f"No-Call ì •í™•ë„ê°€ ë‚®ì•„ ë¶ˆí•„ìš”í•œ tool í˜¸ì¶œ ë°œìƒ")
        w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. "ëª‡ í„´ê¹Œì§€ ë²„í‹°ë‚˜?" â€” ì„±ëŠ¥ ê³¡ì„ 
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  3. ì„±ëŠ¥ ê³¡ì„  â€” ëª‡ í„´ê¹Œì§€ 85%ë¥¼ ìœ ì§€í•˜ëŠ”ê°€?")
    w("=" * 78)
    w("")
    w("  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ T3~T19 ì§€ì ì—ì„œ ì˜ë¼ ëˆ„ì  í‰ê· ì„ ê³„ì‚°í•œë‹¤.")
    w("  ğŸŸ¢ 90%+ | ğŸ”µ 85%+ | ğŸŸ¡ 75%+ | ğŸ”´ <75%  (85% = ì ˆëŒ€ ì„ê³„ì„ )")
    w("")

    w("  [3a] Performance ì¢…í•©")
    _turnpoint_table(w, "Performance", cumul_perf, models, as_zone=True)
    w("")

    w("  [3b] Tool Name Acc")
    _turnpoint_table(w, "Tool", cumul_tool, models, as_zone=False)
    w("")

    w("  [3c] Arg Value Acc")
    _turnpoint_table(w, "Arg", cumul_arg, models, as_zone=False)
    w("")

    w("  [3d] FC Judgment")
    _turnpoint_table(w, "FC", cumul_fc, models, as_zone=False)
    w("")

    # [3e] Stressë³„ Performance ê³¡ì„ 
    st_names = {"ST1": "ì¡°ê±´ëˆ„ì ", "ST2": "ë§¥ë½í¬ì„", "ST3": "êµë€ì£¼ì…"}
    w("  [3e] Stressë³„ Performance ê³¡ì„ ")
    w("  â†’ ë™ì¼ turn-pointì—ì„œ ì–´ë–¤ ìŠ¤íŠ¸ë ˆìŠ¤ ìœ í˜•ì´ ë¨¼ì € ì„±ëŠ¥ì„ ê¹ëŠ”ì§€ ë¹„êµ")
    w("")
    for st in ("ST1", "ST2", "ST3"):
        cumul_st = {m: cumul_perf_st[m][st] for m in models}
        w(f"    [{st} â€” {st_names[st]}]")
        _turnpoint_table(w, st, cumul_st, models, as_zone=True)
        w("")

    # ST ê°„ ìµœëŒ€ í¸ì°¨ê°€ í° ëª¨ë¸ ì‹ë³„
    w("    [Stress ë¯¼ê°ë„ ìš”ì•½]")
    w(f"      {'ëª¨ë¸':<28} {'ST1':>6} {'ST2':>6} {'ST3':>6} {'ìµœëŒ€í¸ì°¨':>8} {'ìµœì•½':>10}")
    w(f"      {'â”€' * 28} {'â”€' * 6} {'â”€' * 6} {'â”€' * 6} {'â”€' * 8} {'â”€' * 10}")
    for model in models:
        if model in unusable:
            continue
        # cross["st_perf"]ë¥¼ ì‚¬ìš©í•˜ì—¬ 4ì ˆê³¼ ë™ì¼í•œ ì†ŒìŠ¤ ë³´ì¥
        st_finals = cross[model]["st_perf"]
        if not st_finals:
            continue
        spread = max(st_finals.values()) - min(st_finals.values())
        worst = min(st_finals, key=st_finals.get)
        w(f"      {_short(model):<28} {st_finals.get('ST1', 0):>5.0%} {st_finals.get('ST2', 0):>5.0%}"
          f" {st_finals.get('ST3', 0):>5.0%} {spread * 100:>5.1f}%p"
          f"  {worst}({st_names.get(worst, '')})")
    w("")

    # â”€â”€ tool ê³¼ì‰ â†” êµë€ ë‚´ì„± ì¸ì‚¬ì´íŠ¸ â”€â”€
    # ST3 no_call ë¹„ì¤‘ì´ ë‚®ìœ¼ë¯€ë¡œ (36í„´ ì¤‘ 4í„´=11%), tool ê³¼ì‰ ëª¨ë¸ì´
    # êµë€(ST3)ì— ì˜¤íˆë ¤ ê°•í•  ìˆ˜ ìˆìŒì„ ë¶„ì„
    aggressive_models = [m for m in usable
                         if overall[m]["tool"] >= 0.70 and sp[m]["nc_acc"] < 0.50]
    if aggressive_models:
        # ST3 vs ë‹¤ë¥¸ STì˜ ì„±ëŠ¥ ë¹„êµ
        st3_stronger = []
        for m in aggressive_models:
            st_f = cross[m]["st_perf"]
            if st_f["ST3"] >= max(st_f["ST1"], st_f["ST2"]):
                st3_stronger.append(m)

        if st3_stronger:
            w("    [ì¸ì‚¬ì´íŠ¸: tool ê³¼ì‰ ì„±í–¥ â†” êµë€ ë‚´ì„±]")
            w(f"      ST3 êµë€ì£¼ì…ì€ tool_call í„´ì´ ~89%ë¥¼ ì°¨ì§€í•œë‹¤.")
            w(f"      'tool ê³¼ì‰' ëª¨ë¸ì€ no_callì— ì•½í•˜ì§€ë§Œ, êµë€ í›„ì—ë„ ì£¼ì € ì—†ì´")
            w(f"      ì˜¬ë°”ë¥¸ toolì„ í˜¸ì¶œí•˜ë¯€ë¡œ ST3 ì„±ëŠ¥ì´ ì˜¤íˆë ¤ ë†’ë‹¤.")
            w("")
            w(f"      {'ëª¨ë¸':<28} {'NCì •ë‹µ':>7} {'ST1':>6} {'ST2':>6} {'ST3':>6} {'ST3ì´ ìµœê³ ?':>12}")
            w(f"      {'â”€' * 28} {'â”€' * 7} {'â”€' * 6} {'â”€' * 6} {'â”€' * 6} {'â”€' * 12}")
            for m in aggressive_models:
                nc = sp[m]["nc_acc"]
                st_f = cross[m]["st_perf"]
                is_best = "âœ“" if m in st3_stronger else ""
                w(f"      {_short(m):<28} {nc:>6.0%} {st_f['ST1']:>5.0%}"
                  f" {st_f['ST2']:>5.0%} {st_f['ST3']:>5.0%} {is_best:>12}")
            # ë°˜ëŒ€ë¡œ ê· í˜•/ë³´ìˆ˜ ëª¨ë¸ì˜ ST3 ì ìˆ˜
            balanced = [m for m in usable
                        if sp[m]["nc_acc"] >= 0.60 and overall[m]["tool"] >= 0.70
                        and m not in aggressive_models]
            if balanced:
                w(f"      â”€â”€â”€ ë¹„êµ: ê· í˜• ëª¨ë¸ â”€â”€â”€")
                for m in balanced:
                    nc = sp[m]["nc_acc"]
                    st_f = cross[m]["st_perf"]
                    w(f"      {_short(m):<28} {nc:>6.0%} {st_f['ST1']:>5.0%}"
                      f" {st_f['ST2']:>5.0%} {st_f['ST3']:>5.0%}")
            w(f"      â†’ tool ê³¼ì‰ ì„±í–¥ì€ êµë€ ë‚´ì„±ì—ì„œ ìœ ë¦¬í•˜ë‚˜,")
            w(f"        no_call ì •í™•ë„ë¥¼ í¬ìƒí•˜ëŠ” trade-offê°€ ì¡´ì¬í•œë‹¤.")
            w("")

    # ë¶•ê´´ ìˆœì„œ
    fc_resilient = sum(
        1 for m in usable
        if cumul_fc[m].get(17, cumul_fc[m].get(15, 0)) > cumul_tool[m].get(17, cumul_tool[m].get(15, 0))
        and cumul_fc[m].get(17, cumul_fc[m].get(15, 0)) > 0.50
    )
    w(f"  ë¶•ê´´ ìˆœì„œ: ì¸ì(Arg) â†’ ë„êµ¬ ì„ íƒ(Tool) â†’ í–‰ë™ íŒë‹¨(FC) ({fc_resilient}/{len(usable)} ëª¨ë¸)")
    w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. "ì™œ ë§ê°€ì§€ë‚˜?" â€” ì›ì¸ ë¶„ì„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  4. ì›ì¸ ë¶„ì„ â€” ë¬´ì—‡ì´ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦¬ëŠ”ê°€?")
    w("=" * 78)
    w("")
    w("  6ê°œ ì‹œë‚˜ë¦¬ì˜¤(O1/O2 Ã— ST1/ST2/ST3)ì—ì„œ 'ì–´ë–¤ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë” ì¹˜ëª…ì ì¸ê°€' ë¹„êµ.")
    w("")

    # Stress Type table with performance
    w("  [ìŠ¤íŠ¸ë ˆìŠ¤ ìœ í˜•ë³„ Performance]")
    w(f"    {'ëª¨ë¸':<28} {'ST1(ëˆ„ì )':>9} {'ST2(í¬ì„)':>9} {'ST3(êµë€)':>9} {'í¸ì°¨':>8}")
    w(f"    {'â”€' * 28} {'â”€' * 9} {'â”€' * 9} {'â”€' * 9} {'â”€' * 8}")
    # STë³„ 1ìœ„ (usable)
    _st_best = {}
    for st in ["ST1", "ST2", "ST3"]:
        _st_best[st] = max(usable, key=lambda m: cross[m]["st_perf"].get(st, 0), default=None) if usable else None

    for model in models:
        d = cross[model]["st_perf"]
        vals = list(d.values())
        spread = max(vals) - min(vals) if vals else 0
        row = f"    {_short(model):<28}"
        for st in ["ST1", "ST2", "ST3"]:
            v = d.get(st, 0)
            mark = " â—€" if model == _st_best.get(st) and model in usable else ""
            row += f" {v:>8.1%}{mark}"
        row += f" {spread * 100:>5.1f}%p"
        w(row)
    w("")

    # Outcome table
    w("  [ì½œ ìœ í˜•ë³„ Tool Acc]")
    w(f"    {'ëª¨ë¸':<28} {'O1(ì²­ì•½)':>9} {'O2(ë³´ë¥˜)':>9} {'ì°¨ì´':>8}")
    w(f"    {'â”€' * 28} {'â”€' * 9} {'â”€' * 9} {'â”€' * 8}")
    for model in models:
        d = cross[model]["outcome_tool"]
        o1 = d.get("O1", 0)
        o2 = d.get("O2", 0)
        diff = abs(o1 - o2)
        w(f"    {_short(model):<28} {o1:>8.1%} {o2:>8.1%} {diff * 100:>5.1f}%p")
    w("")

    # ëª¨ë¸ë³„ ìµœì•½ ST ì§‘ê³„
    _worst_cnt = defaultdict(int)
    for m in usable:
        _d = cross[m]["st_perf"]
        if _d:
            _worst_cnt[min(_d, key=_d.get)] += 1
    _dominant_worst = max(_worst_cnt, key=_worst_cnt.get) if _worst_cnt else st_order[-1]
    w(f"  â†’ ê°€ì¥ ì¹˜ëª…ì : {st_names[_dominant_worst]}({_dominant_worst}) "
      f"â€” usable {len(usable)}ê°œ ëª¨ë¸ ì¤‘ {_worst_cnt[_dominant_worst]}ê°œê°€ ìµœì•½")
    w(f"    ì²­ì•½ vs ë³´ë¥˜ ì°¨ì´: í‰ê·  {avg_outcome_diff*100:.1f}%p(ë¯¸ë¯¸) "
      f"â†’ ë³€ë³„ë ¥ì€ ìŠ¤íŠ¸ë ˆìŠ¤ ìœ í˜•(ST1/ST2/ST3)ì— ìˆìŒ")
    w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4b. Error Taxonomy â€” ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  4b. Error Taxonomy â€” ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜")
    w("=" * 78)
    w("")
    w("  ê° í„´ì˜ ì‹¤íŒ¨ë¥¼ 6ê°œ íƒœê·¸ë¡œ ë¶„ë¥˜í•˜ì—¬ 'ì–´ë–¤ ì¢…ë¥˜ì˜ ì‹¤ìˆ˜ë¥¼ í•˜ëŠ”ê°€' ì§„ë‹¨.")
    w("  ê°œì„  ë°©í–¥: ê° ì—ëŸ¬ ìœ í˜•ì˜ Top íƒœê·¸ë¥¼ ìš°ì„  ê°œì„ .")
    w("")
    w("  íƒœê·¸ ì •ì˜:")
    for tag, desc in ERROR_TAGS.items():
        w(f"    {tag:<14} {desc}")
    w("")

    # ì—ëŸ¬ í…Œì´ë¸”
    et_cols = [28, 10, 10, 10, 10, 10, 10, 8]
    et_sep = "  " + "+".join("-" * c for c in et_cols) + "+"
    w(et_sep)
    w(f"  | {'ëª¨ë¸':<{et_cols[0]-2}}"
      f"| {'WRONG':^{et_cols[1]-1}}"
      f"| {'MISSED':^{et_cols[2]-1}}"
      f"| {'FALSE':^{et_cols[3]-1}}"
      f"| {'ARG_MIS':^{et_cols[4]-1}}"
      f"| {'ARG_WR':^{et_cols[5]-1}}"
      f"| {'STALE':^{et_cols[6]-1}}"
      f"| {'OK':^{et_cols[7]-1}}|")
    w(f"  | {'':^{et_cols[0]-2}}"
      f"| {'_TOOL':^{et_cols[1]-1}}"
      f"| {'_CALL':^{et_cols[2]-1}}"
      f"| {'_CALL':^{et_cols[3]-1}}"
      f"| {'SING':^{et_cols[4]-1}}"
      f"| {'ONG':^{et_cols[5]-1}}"
      f"| {'(ST3)':^{et_cols[6]-1}}"
      f"| {'':^{et_cols[7]-1}}|")
    w(et_sep)

    for model in models:
        d = err_tax[model]
        short = _short(model)[:et_cols[0]-2]
        total = d["_total"]
        ok = d["_correct"]
        w(f"  | {short:<{et_cols[0]-2}}"
          f"| {d['WRONG_TOOL']:^{et_cols[1]-1}}"
          f"| {d['MISSED_CALL']:^{et_cols[2]-1}}"
          f"| {d['FALSE_CALL']:^{et_cols[3]-1}}"
          f"| {d['ARG_MISSING']:^{et_cols[4]-1}}"
          f"| {d['ARG_WRONG']:^{et_cols[5]-1}}"
          f"| {d['ARG_STALE']:^{et_cols[6]-1}}"
          f"| {ok:^{et_cols[7]-1}}|")
    w(et_sep)
    w("")

    # ëª¨ë¸ë³„ Top-2 ì—ëŸ¬ íƒœê·¸ + ê°œì„  ë°©í–¥
    w("  [ëª¨ë¸ë³„ Top ì—ëŸ¬ + ê°œì„  ë°©í–¥]")
    tag_fixes = {
        "WRONG_TOOL": "tool description ê°œì„  ë˜ëŠ” tool ì„ íƒ ì •í™•ë„ í–¥ìƒ",
        "MISSED_CALL": "í˜¸ì¶œ íŒë‹¨ ê¸°ì¤€ ê°•í™”",
        "FALSE_CALL": "No-Call íŒë³„ ì •í™•ë„ í–¥ìƒ",
        "ARG_MISSING": "í•„ìˆ˜ ì¸ì ì±„ì›€ ë¡œì§ ë³´ê°•",
        "ARG_WRONG": "ì¸ì ê°’ ì •í™•ë„ í–¥ìƒ",
        "ARG_STALE": "ëŒ€í™” ìƒíƒœ ì¶”ì /ê°±ì‹  ë³´ê°•",
    }
    for model in models:
        if model in unusable:
            continue
        d = err_tax[model]
        errs = [(tag, d[tag]) for tag in ERROR_TAGS if d[tag] > 0]
        errs.sort(key=lambda x: x[1], reverse=True)
        top2 = errs[:2]
        short = _short(model)
        if top2:
            top_str = " > ".join(f"{tag}({cnt})" for tag, cnt in top2)
            fix = tag_fixes.get(top2[0][0], "")
            w(f"    {short:<28} {top_str}")
            w(f"    {'':28} â†’ {fix}")
        else:
            w(f"    {short:<28} ì—ëŸ¬ ì—†ìŒ")
    w("")

    # â”€â”€ 1ìœ„ ëª¨ë¸ ì—ëŸ¬ ì¸ì‚¬ì´íŠ¸ â”€â”€
    _best_err = err_tax[best_perf_model]
    _best_total = _best_err["_total"]
    _best_ok = _best_err["_correct"]
    _best_arg_total = _best_err["ARG_WRONG"] + _best_err["ARG_STALE"]
    _best_errs = [(tag, _best_err[tag]) for tag in ERROR_TAGS if _best_err[tag] > 0]
    _best_errs.sort(key=lambda x: x[1], reverse=True)

    w(f"  [1ìœ„ ëª¨ë¸({_short(best_perf_model)}) ì—ëŸ¬ ì¸ì‚¬ì´íŠ¸]")
    w(f"    ì „ì²´ {_best_total}í„´ ì¤‘ ì™„ë²½ ì •ë‹µ {_best_ok}í„´ ({_best_ok/_best_total:.0%})")
    w("")
    w(f"    â— í•µì‹¬ ì•½ì  â€” ì¸ì ì˜¤ë¥˜ {_best_arg_total}ê±´ ({_best_arg_total/_best_total:.0%})")
    w(f"      ARG_WRONG({_best_err['ARG_WRONG']}) + ARG_STALE({_best_err['ARG_STALE']}):")
    w(f"      toolì€ ë§ê²Œ ê³¨ëì§€ë§Œ ì¸ì ê°’ì„ í‹€ë¦¼.")
    if _best_err["ARG_STALE"] > 0:
        w(f"      íŠ¹íˆ STALE {_best_err['ARG_STALE']}ê±´ì€ ê³ ê°ì´ ê°’ì„ ë²ˆë³µí•œ ë’¤")
        w(f"      ì´ì „ ê°’ì„ ê°±ì‹ í•˜ì§€ ëª»í•œ ì‹¤ìˆ˜ â†’ ëŒ€í™” ìƒíƒœ ì¶”ì  ì‹¤íŒ¨.")
    w("")
    if _best_err["WRONG_TOOL"] > 0:
        w(f"    â— tool í˜¼ë™ â€” WRONG_TOOL {_best_err['WRONG_TOOL']}ê±´")
        w(f"      ì •ë‹µê³¼ ìœ ì‚¬í•œ ë‹¤ë¥¸ toolì„ í˜¸ì¶œí•˜ëŠ” ì‹¤ìˆ˜.")
        w("")
    if _best_err["FALSE_CALL"] > 0:
        w(f"    â— No-Call ì‹¤íŒ¨ â€” FALSE_CALL {_best_err['FALSE_CALL']}ê±´")
        w(f"      ì •ë³´ ë¶€ì¡±/ë²”ìœ„ ë°– ìƒí™©ì—ì„œ toolì„ í˜¸ì¶œí•´ë²„ë¦¼.")
        w("")
    if _best_err["MISSED_CALL"] == 0:
        w(f"    â— ê°•ì  â€” MISSED_CALL 0ê±´")
        w(f"      í˜¸ì¶œí•´ì•¼ í•  ë•Œ ë¹ ëœ¨ë¦¬ëŠ” ì¼ì€ í•œ ë²ˆë„ ì—†ìŒ.")
        w(f"      toolì„ ì ê·¹ì ìœ¼ë¡œ ë¶€ë¥´ëŠ” ì„±í–¥ì´ êµë€(ST3) ë‚´ì„±ì˜ ì›ì¸.")
        w("")
    # í•œ ì¤„ ìš”ì•½ â€” top ì—ëŸ¬ ê¸°ë°˜ ë™ì  ìƒì„±
    _top_err = _best_errs[0] if _best_errs else None
    if _top_err:
        _err_to_lever = {
            "ARG_WRONG": "Arg Acc", "ARG_STALE": "Arg Acc",
            "ARG_MISSING": "Arg Acc", "WRONG_TOOL": "Tool Acc",
            "MISSED_CALL": "Tool Acc", "FALSE_CALL": "No-Call",
        }
        _lever = _err_to_lever.get(_top_err[0], _top_err[0])
        w(f"    í•œ ì¤„ ìš”ì•½: ìµœë‹¤ ì—ëŸ¬ëŠ” {_top_err[0]}({_top_err[1]}ê±´) â†’ {_lever} ê°œì„ ì´ ìµœìš°ì„ .")
    w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5. ëª¨ë¸ë³„ íŒì • (ìì—°ì–´)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  5. ëª¨ë¸ë³„ íŒì •")
    w("=" * 78)
    w("")

    # ìˆœìœ„ ì •ë ¬ (usable â†’ prod_perf ë‚´ë¦¼ì°¨ìˆœ, unusableì€ ë)
    _ranked = sorted(usable, key=lambda m: prod_perf[m], reverse=True)
    _ranked += [m for m in models if m in unusable]

    for rank_idx, model in enumerate(_ranked):
        short = _short(model)
        o = overall[model]
        d = sp[model]
        pp = prod_perf[model]
        safe_t = safe_turns[model]
        e = err_tax[model]

        if model in unusable:
            # â”€â”€ ì‚¬ìš© ë¶ˆê°€ ëª¨ë¸ â”€â”€
            w(f"  âŒ {short} â€” ì‚¬ìš© ë¶ˆê°€ (ì‹¤ë¬´ {pp:.0%})")
            # ì™œ ì‚¬ìš© ë¶ˆê°€ì¸ì§€
            if o["tool"] < 0.15:
                w(f"    tool í˜¸ì¶œ ìì²´ë¥¼ ê±°ì˜ ëª» í•¨ (Tool {o['tool']:.0%}).")
                if d["nc_acc"] >= 0.90:
                    w(f"    No-Call {d['nc_acc']:.0%}ëŠ” toolì„ ëª» ë¶ˆëŸ¬ì„œ ë†’ì€ ê²ƒì´ì§€,")
                    w(f"    íŒë‹¨ì´ ì¢‹ì€ ê²Œ ì•„ë‹˜.")
            else:
                w(f"    ì „ì²´ Performance {o['performance']:.0%}ë¡œ ì‹¤ë¬´ íˆ¬ì… ê¸°ì¤€ ë¯¸ë‹¬.")
            w("")
            continue

        # â”€â”€ íŒì • ë¼ë²¨ â”€â”€
        if model == best_perf_model:
            label = "ê¶Œì¥"
            icon = "ğŸ†"
        elif rank_idx == 1:
            label = "ì°¨ì„ "
            icon = "  "
        elif pp >= 0.70:
            label = "ì¡°ê±´ë¶€ ì‚¬ìš©"
            icon = "  "
        else:
            label = "ë¹„ê¶Œì¥"
            icon = "  "

        w(f"  {icon} {short} â€” {label} (ì‹¤ë¬´ {pp:.0%})")

        # â”€â”€ ê°•ì  â”€â”€
        strengths = []
        if model == best_perf_model:
            strengths.append(f"ì‹¤ë¬´ {pp:.0%}ë¡œ 1ìœ„")
        if e["MISSED_CALL"] == 0 and o["performance"] >= 0.50:
            strengths.append("í˜¸ì¶œ ëˆ„ë½ 0ê±´ â€” toolì„ ì ê·¹ì ìœ¼ë¡œ ë¶€ë¦„")
        nl_q = o.get("nl_quality")
        if nl_q is not None and nl_q >= 0.80:
            strengths.append(f"NL {nl_q:.0%}ë¡œ ë‹µë³€ í’ˆì§ˆ ìš°ìˆ˜ â†’ Agent+ë‹µë³€ ê²¸ìš© ê°€ëŠ¥")
        if d["nc_acc"] >= 0.70:
            strengths.append(f"No-Call {d['nc_acc']:.0%}ë¡œ ìƒí™© íŒë‹¨ì´ ì •í™• (ê· í˜•í˜•)")

        # â”€â”€ ì•½ì  â”€â”€
        weaknesses = []
        arg_err = e["ARG_WRONG"] + e["ARG_STALE"]
        if arg_err >= 15:
            stale_note = f"(ë²ˆë³µ ë¯¸ê°±ì‹  {e['ARG_STALE']}ê±´ í¬í•¨)" if e["ARG_STALE"] > 5 else ""
            weaknesses.append(f"ì¸ì ì˜¤ë¥˜ {arg_err}ê±´{stale_note} â€” ê°’ì„ ì±„ìš°ëŠ” ì •ë°€ë„ ë¶€ì¡±")
        if e["WRONG_TOOL"] >= 15:
            weaknesses.append(f"tool í˜¼ë™ {e['WRONG_TOOL']}ê±´ â€” ìœ ì‚¬ tool ê°„ êµ¬ë¶„ ì‹¤íŒ¨")
        if d["nc_acc"] < 0.50 and o["tool"] >= 0.70:
            weaknesses.append(f"No-Call {d['nc_acc']:.0%} â€” ë¶ˆëŸ¬ì•¼/ë§ì•„ì•¼ íŒë‹¨ ë¶€ì¡± (tool ê³¼ì‰)")
        if o["tool"] - o["arg"] > 0.35:
            weaknesses.append(f"Tool {o['tool']:.0%} vs Arg {o['arg']:.0%} â€” toolì€ ë§ì¶”ì§€ë§Œ ì¸ìë¥¼ ì ˆë°˜ ì´ìƒ í‹€ë¦¼")
        if d["parallel_detect"] == 0 and o["performance"] >= 0.50:
            weaknesses.append("ë³µìˆ˜í˜¸ì¶œ ì¸ì‹ 0%")

        # â”€â”€ ìŠ¤íŠ¸ë ˆìŠ¤ ì•½ì  â”€â”€
        _st_d = cross[model]["st_perf"]
        if _st_d:
            _st_worst = min(_st_d, key=_st_d.get)
            _st_best = max(_st_d, key=_st_d.get)
            _st_spread = _st_d[_st_best] - _st_d[_st_worst]
            if _st_spread > 0.10:
                weaknesses.append(
                    f"{st_names[_st_worst]}({_st_worst})ì— ì•½í•¨ "
                    f"({_st_d[_st_worst]:.0%}, ìµœê°• {st_names[_st_best]} {_st_d[_st_best]:.0%}ê³¼ "
                    f"{_st_spread*100:.0f}%p ì°¨ì´)")

        # â”€â”€ ì¶œë ¥ â”€â”€
        if strengths:
            for s in strengths:
                w(f"    + {s}")
        if weaknesses:
            for wk in weaknesses:
                w(f"    - {wk}")

        # â”€â”€ ë¹„ê¶Œì¥ ì‚¬ìœ  â”€â”€
        if label == "ë¹„ê¶Œì¥":
            w(f"    â†’ ì‹¤ë¬´ íˆ¬ì… ì‹œ ë¦¬ìŠ¤í¬ê°€ ë†’ìŒ.")

        w("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6. ê²°ë¡ 
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    w("=" * 78)
    w("  6. ê²°ë¡ ")
    w("=" * 78)
    w("")

    best_pp = prod_perf[best_perf_model]
    best_fp = overall[best_perf_model]["performance"]
    par_best = max(sp[m]["parallel_detect"] for m in models) if models else 0

    # â”€â”€ í˜„ì¬ ìœ„ì¹˜ â”€â”€
    w(f"  [í˜„ì¬ ìœ„ì¹˜ â€” {_short(best_perf_model)} @T{PRODUCTION_CUTOFF}]")
    w(f"    Performance {best_pp:.0%} = "
      f"Tool {_bp_prod_tool:.0%} + Arg {_bp_prod_arg:.0%} + "
      f"FC {_bp_prod_fc:.0%} (toolí„´) / NC {_bp_prod_nc:.0%} (no-callí„´)")
    w(f"    tool_call {_bp_tc_n}í„´ ({_bp_tc_n/_bp_total_n:.0%}) + "
      f"no_call {_bp_nc_n}í„´ ({_bp_nc_n/_bp_total_n:.0%})")
    w("")

    # â”€â”€ ë¯¼ê°ë„ ë¶„ì„ â”€â”€
    w(f"  [ë¯¼ê°ë„ ë¶„ì„ â€” ì–´ë””ë¥¼ ê³ ì¹˜ë©´ Performanceê°€ ê°€ì¥ ì˜¤ë¥´ëŠ”ê°€?]")
    sens_per10 = lambda n: (0.10 * n) / _bp_total_n  # +10%p ê°œì„  ì‹œ Perf ë³€í™”
    sens = [
        ("Arg Acc", _bp_prod_arg, sens_per10(_bp_tc_n / 3),
         max(0, 0.95 - _bp_prod_arg)),
        ("No-Call", _bp_prod_nc, sens_per10(_bp_nc_n),
         max(0, 0.85 - _bp_prod_nc)),
        ("Tool Acc", _bp_prod_tool, sens_per10(_bp_tc_n / 3),
         max(0, 0.98 - _bp_prod_tool)),
    ]
    w(f"    {'ì§€í‘œ':<12} {'í˜„ì¬':>6} {'ì—¬ìœ ':>8} {'ë¯¼ê°ë„':>8} {'ìµœëŒ€ íš¨ê³¼':>10} {'ìš°ì„ ìˆœìœ„':>8}")
    w(f"    {'â”€' * 12} {'â”€' * 6} {'â”€' * 8} {'â”€' * 8} {'â”€' * 10} {'â”€' * 8}")
    for name, cur, s_per10, headroom in sens:
        headroom_str = f"+{headroom*100:.0f}%p" if headroom > 0 else "í¬í™”"
        # ìµœëŒ€ íš¨ê³¼ = ë¯¼ê°ë„ Ã— ì—¬ìœ  (í•´ë‹¹ ì§€í‘œë¥¼ ì²œì¥ê¹Œì§€ ì˜¬ë ¸ì„ ë•Œ Perf ë³€í™”)
        max_gain = s_per10 * (headroom / 0.10) if headroom > 0 else 0
        prio = "â˜…â˜…â˜…" if max_gain >= 0.04 else ("â˜…â˜…" if max_gain >= 0.01 else "â˜…")
        w(f"    {name:<12} {cur:>5.0%} {headroom_str:>8}"
          f" {s_per10*100:>+6.1f}%p {max_gain*100:>+8.1f}%p {prio:>8}")
    w(f"    ë¯¼ê°ë„=+10%pë‹¹ Perf ë³€í™” | ìµœëŒ€ íš¨ê³¼=ì—¬ìœ ë¶„ ì „ë¶€ ê°œì„  ì‹œ Perf ë³€í™”")
    # top-2 ë ˆë²„ë¥¼ ìµœëŒ€ íš¨ê³¼ ê¸°ì¤€ìœ¼ë¡œ ë™ì  ì„ íƒ
    _sens_ranked = sorted(sens, key=lambda x: x[2] * (x[3] / 0.10) if x[3] > 0 else 0, reverse=True)
    _top_levers = [s[0] for s in _sens_ranked[:2] if s[3] > 0]
    if _top_levers:
        w(f"    â†’ {'ì™€ '.join(_top_levers)}ì´ ê°€ì¥ íš¨ê³¼ì ì¸ ê°œì„  ë ˆë²„")
    w("")

    # â”€â”€ ìš´ì˜ ê°€ì´ë“œ â”€â”€
    w(f"  [ìš´ì˜ ê°€ì´ë“œ]")
    w(f"    â€¢ í„´ ì œí•œ: ì‹¤ë¬´ {PRODUCTION_CUTOFF}í„´ ì´ë‚´ (í˜„ì¬ {best_pp:.0%}, ì¶©ë¶„íˆ í™œìš© ê°€ëŠ¥)")
    w(f"    â€¢ T{PRODUCTION_CUTOFF} ì´í›„ ì„±ëŠ¥ í•˜ë½ì€ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ì´ë©°, ìš´ì˜ ëª©í‘œ ì•„ë‹˜")
    w(f"    â€¢ ê°œì„  í›„ ì´ ë²¤ì¹˜ë§ˆí¬ ì¬ì‹¤í–‰ â†’ ë‹¬ì„± ì—¬ë¶€ í™•ì¸")
    w("")
    w("=" * 78)

    # íŒŒì¼ ì €ì¥
    text = "\n".join(lines)
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(text)

    # ì½˜ì†”ì—ë„ ì¶œë ¥
    print(text)
    print(f"\n    report: {save_path.name}")


def generate_report_md(
    meta: dict,
    results: dict,
    save_path: Path,
):
    """GitHub ë Œë”ë§ìš© Markdown ë¦¬í¬íŠ¸."""
    lines: list[str] = []
    w = lines.append

    models = list(results.keys())
    overall = compute_overall(results)
    sp = compute_single_parallel(results)
    cumul_perf = compute_turnpoint_performance(results)
    err_tax = compute_error_taxonomy(results)
    cross = compute_stress_cross_analysis(results)

    prod_perf: dict[str, float] = {}
    for model in models:
        prod_perf[model] = cumul_perf.get(model, {}).get(PRODUCTION_CUTOFF, 0)

    # safe turns
    safe_turns: dict[str, int] = {}
    for model in models:
        _vals = cumul_perf[model]
        _d85 = None
        for _c in TURN_CUTOFFS:
            _v = _vals.get(_c)
            if _v is not None and _d85 is None and _v < THRESHOLD_CRITICAL:
                _d85 = _c
        if _d85:
            _idx = TURN_CUTOFFS.index(_d85)
            safe_turns[model] = TURN_CUTOFFS[_idx - 1] if _idx > 0 else 0
        else:
            safe_turns[model] = max(TURN_CUTOFFS)

    usable = [m for m in models if overall[m]["performance"] >= 0.30]
    git_rev = _get_git_rev()

    # â”€â”€ Header â”€â”€
    w("# AI TMR Assistant â€” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
    w("")
    w(f"> ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M')} | "
      f"í„´: {meta.get('total_turns', '?')} Ã— {len(models)}ëª¨ë¸ | "
      f"Judge: {meta.get('judge_model', 'N/A')}  ")
    w(f"> commit: `{git_rev}` | run_id: `{meta.get('run_id', '?')}`  ")
    for cfg_line in _format_config(meta):
        w(f"> config:{cfg_line}")
    w("")

    # â”€â”€ ìš©ì–´ ì •ì˜ â”€â”€
    w("> **@T7 (ì‹¤ë¬´ êµ¬ê°„)** = Turn 1\~7ê¹Œì§€ì˜ ëˆ„ì  ì„±ëŠ¥. TMR ì˜ì—…ì½œì˜ "
      "ì‹¤ë¬´ í„´ ìˆ˜(ì²­ì•½ \~7í„´, ë³´ë¥˜ \~5í„´)ì— ëŒ€ì‘í•˜ëŠ” ìš´ì˜ ê¸°ì¤€ì„ . "
      "T7 ì´í›„(T10\~T19)ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ êµ¬ê°„(ë‚´êµ¬ë„ ì§„ë‹¨ìš©).")
    w("")

    # â”€â”€ 1. ì„±ì í‘œ â”€â”€
    w("## 1. ëª¨ë¸ë³„ ì„±ì í‘œ")
    w("")
    w("| ëª¨ë¸ | Tool | Arg | FC | NL | ì‹¤ë¬´ Perf | ì „ì²´ Perf |")
    w("|------|------|-----|----|----|-----------|-----------|")
    for model in models:
        o = overall[model]
        pp = prod_perf[model]
        short = _short(model)
        nl_str = f"{o['nl_quality']:.0%}" if o["nl_quality"] is not None else "N/A"
        w(f"| {short} | {o['tool']:.1%} | {o['arg']:.1%} | "
          f"{o['fc']:.1%} | {nl_str} | {pp:.0%} | {o['performance']:.0%} |")
    w("")

    # â”€â”€ 2. ëŠ¥ë ¥ í•´ë¶€ â”€â”€
    w("## 2. ëŠ¥ë ¥ í•´ë¶€ â€” Single / Parallel / No-Call")
    w("")
    w("| ëª¨ë¸ | S:Tool | S:Arg | P:Tool | P:Arg | P:ê°ì§€ | NC:Acc |")
    w("|------|--------|-------|--------|-------|--------|--------|")
    for model in models:
        d = sp[model]
        w(f"| {_short(model)} | {d['single_tool']:.0%} | {d['single_arg']:.0%} | "
          f"{d['parallel_tool']:.0%} | {d['parallel_arg']:.0%} | "
          f"{d['parallel_detect']:.0%} | {d['nc_acc']:.0%} |")
    w("")

    # â”€â”€ 3. ì„±ëŠ¥ ê³¡ì„  â”€â”€
    w("## 3. ì„±ëŠ¥ ê³¡ì„  â€” Turn-Point Performance")
    w("")
    w("ğŸŸ¢ 90%+ | ğŸ”µ 85%+ | ğŸŸ¡ 75%+ | ğŸ”´ <75%")
    w("")
    header = "| ëª¨ë¸ |" + " | ".join(f"~T{c}" for c in TURN_CUTOFFS) + " |"
    sep_row = "|------|" + " | ".join("---:" for _ in TURN_CUTOFFS) + " |"
    w(header)
    w(sep_row)
    for model in models:
        row = f"| {_short(model)} |"
        for c in TURN_CUTOFFS:
            v = cumul_perf[model].get(c)
            if v is None:
                row += " - |"
            else:
                row += f" {_zone_dot(v)} |"
        w(row)
    w("")

    # â”€â”€ 4. ìŠ¤íŠ¸ë ˆìŠ¤ ë¯¼ê°ë„ â”€â”€
    w("## 4. ìŠ¤íŠ¸ë ˆìŠ¤ ë¯¼ê°ë„")
    w("")
    w("| ëª¨ë¸ | ST1(ì¡°ê±´ëˆ„ì ) | ST2(ë§¥ë½í¬ì„) | ST3(êµë€ì£¼ì…) | ìµœì•½ì  |")
    w("|------|-------------|-------------|-------------|--------|")
    for model in models:
        if model not in usable:
            continue
        d = cross[model]["st_perf"]
        vals = list(d.values())
        worst = min(d, key=d.get) if d else "-"
        st_names_map = {"ST1": "ì¡°ê±´ëˆ„ì ", "ST2": "ë§¥ë½í¬ì„", "ST3": "êµë€ì£¼ì…"}
        w(f"| {_short(model)} | {d.get('ST1', 0):.0%} | {d.get('ST2', 0):.0%} | "
          f"{d.get('ST3', 0):.0%} | {worst} |")
    w("")

    # â”€â”€ 4b. Error Taxonomy â”€â”€
    w("## 4b. Error Taxonomy")
    w("")
    w("ê° í„´ì˜ ì‹¤íŒ¨ë¥¼ 6ê°œ íƒœê·¸ë¡œ ë¶„ë¥˜.")
    w("")
    w("| íƒœê·¸ | ì„¤ëª… |")
    w("|------|------|")
    for tag, desc in ERROR_TAGS.items():
        w(f"| `{tag}` | {desc} |")
    w("")

    w("| ëª¨ë¸ | WRONG_TOOL | MISSED_CALL | FALSE_CALL | ARG_MISSING | ARG_WRONG | ARG_STALE | OK |")
    w("|------|-----------|------------|-----------|------------|----------|----------|---|")
    for model in models:
        d = err_tax[model]
        ok = d["_correct"]
        w(f"| {_short(model)} | {d['WRONG_TOOL']} | {d['MISSED_CALL']} | "
          f"{d['FALSE_CALL']} | {d['ARG_MISSING']} | {d['ARG_WRONG']} | "
          f"{d['ARG_STALE']} | {ok} |")
    w("")

    # Top ì—ëŸ¬
    tag_fixes = {
        "WRONG_TOOL": "tool ì„ íƒ ì •í™•ë„ í–¥ìƒ",
        "MISSED_CALL": "í˜¸ì¶œ íŒë‹¨ ê¸°ì¤€ ê°•í™”",
        "FALSE_CALL": "No-Call íŒë³„ ì •í™•ë„ í–¥ìƒ",
        "ARG_MISSING": "í•„ìˆ˜ ì¸ì ì±„ì›€ ë³´ê°•",
        "ARG_WRONG": "ì¸ì ê°’ ì •í™•ë„ í–¥ìƒ",
        "ARG_STALE": "ëŒ€í™” ìƒíƒœ ì¶”ì  ë³´ê°•",
    }
    w("**ëª¨ë¸ë³„ Top ì—ëŸ¬:**")
    w("")
    for model in models:
        if overall[model]["performance"] < 0.30:
            continue
        d = err_tax[model]
        errs = [(tag, d[tag]) for tag in ERROR_TAGS if d[tag] > 0]
        errs.sort(key=lambda x: x[1], reverse=True)
        if errs:
            top = errs[0]
            w(f"- **{_short(model)}**: `{top[0]}`({top[1]}) â†’ {tag_fixes.get(top[0], '')}")
    w("")

    # â”€â”€ 5. ìš´ì˜ ê°€ì´ë“œ â”€â”€
    w("## 5. ìš´ì˜ ê°€ì´ë“œ")
    w("")
    best_model = max(usable, key=lambda m: prod_perf[m]) if usable else models[0]
    best_pp = prod_perf[best_model]
    best_safe = safe_turns.get(best_model, 0)
    best_safe_str = f"~T{best_safe}" if best_safe > 0 else "T3 ë¯¸ë§Œ"
    w(f"- **ê¶Œì¥ ëª¨ë¸**: {_short(best_model)} (ì‹¤ë¬´ {best_pp:.0%})")
    w(f"- **ê¶Œì¥ í„´ ì œí•œ**: {PRODUCTION_CUTOFF}í„´ ì´ë‚´")
    w(f"- **85%+ ìœ ì§€ êµ¬ê°„**: {best_safe_str}")
    w(f"- ê°œì„  í›„ ë²¤ì¹˜ë§ˆí¬ ì¬ì‹¤í–‰ìœ¼ë¡œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸")
    w("")

    # ì €ì¥
    text = "\n".join(lines)
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"    report: {save_path.name}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(description="Phase 4: ê²°ê³¼ ë¹„êµ & ì„±ëŠ¥ ê³¡ì„ ")
    parser.add_argument("--run-id", type=str, help="ë¶„ì„í•  run_id (ë¶€ë¶„ ì¼ì¹˜)")
    parser.add_argument("--charts", action="store_true", help="ì°¨íŠ¸ ìƒì„± (ê¸°ë³¸: ë¦¬í¬íŠ¸ë§Œ)")
    parser.add_argument("--list", action="store_true", help="ì €ì¥ëœ ê²°ê³¼ ëª©ë¡")
    args = parser.parse_args()

    if args.list:
        list_results()
        return

    # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    if args.run_id:
        detail_path = find_detail_by_id(args.run_id)
    else:
        detail_path = find_latest_detail()

    if not detail_path:
        print("  ERROR: ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("  ë¨¼ì € python -m benchmark.run_benchmark ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)

    print(f"\n  Loading: {detail_path.name}")

    meta, results = load_detail(detail_path)
    models = list(results.keys())
    run_id = meta.get("run_id", detail_path.stem.replace("detail_", ""))

    print(f"  Models : {len(models)}")
    print(f"  Run ID : {run_id}")
    print()

    # â”€â”€ ì°¨íŠ¸ ìƒì„± (--charts ëª…ì‹œ ì‹œì—ë§Œ) â”€â”€
    if args.charts:
        CHARTS_DIR.mkdir(parents=True, exist_ok=True)

        print("  Generating charts...")

        # [1] Turn-point Performance ì¢…í•©
        cumul_perf = compute_turnpoint_performance(results)
        chart_turnpoint_curve(
            cumul_perf,
            title="Turn-Point: Performance Score  (Tool + Arg + FC) / 3",
            ylabel="Cumulative Performance",
            save_path=CHARTS_DIR / f"turnpoint_performance_{run_id}.png",
        )

        # [2] Turn-point ëˆ„ì  Tool Name Acc (no_call ì œì™¸)
        cumul_tool = compute_turnpoint(results, "bfcl.tool_name_acc", exclude_no_call=True)
        chart_turnpoint_curve(
            cumul_tool,
            title="Turn-Point: Cumulative Tool Name Accuracy",
            ylabel="Cumulative Tool Name Acc",
            save_path=CHARTS_DIR / f"turnpoint_tool_acc_{run_id}.png",
        )

        # [3] Turn-point ëˆ„ì  Arg Value Acc (no_call ì œì™¸)
        cumul_arg = compute_turnpoint(results, "bfcl.arg_value_acc", exclude_no_call=True)
        chart_turnpoint_curve(
            cumul_arg,
            title="Turn-Point: Cumulative Arg Value Accuracy",
            ylabel="Cumulative Arg Value Acc",
            save_path=CHARTS_DIR / f"turnpoint_arg_acc_{run_id}.png",
        )

        # [4] ê°œë³„ í„´ ì •í™•ë„ (no_call ì œì™¸)
        per_turn = compute_per_turn(results, "bfcl.tool_name_acc", exclude_no_call=True)
        chart_per_turn(
            per_turn,
            title="Per-Turn Tool Name Accuracy (Collapse Detection)",
            ylabel="Tool Name Acc at Turn N",
            save_path=CHARTS_DIR / f"per_turn_tool_acc_{run_id}.png",
        )

        # [5] Single vs Parallel
        sp = compute_single_parallel(results)
        chart_single_vs_parallel(
            sp,
            save_path=CHARTS_DIR / f"single_vs_parallel_{run_id}.png",
        )

        # [6] ì‹œë‚˜ë¦¬ì˜¤ íˆíŠ¸ë§µ
        matrix = compute_scenario_matrix(results)
        chart_scenario_heatmap(
            matrix,
            save_path=CHARTS_DIR / f"scenario_heatmap_{run_id}.png",
        )

        # [7] Stressë³„ Performance ê³¡ì„  (3-subplot)
        cumul_perf_st = compute_turnpoint_performance_by_stress(results)
        chart_stress_turnpoint(
            cumul_perf_st,
            save_path=CHARTS_DIR / f"stress_performance_{run_id}.png",
        )

        print()

    # â”€â”€ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ + Markdown ë¦¬í¬íŠ¸ â”€â”€
    print("  Generating reports...")
    report_path = RESULTS_DIR / f"report_{run_id}.txt"
    generate_report(meta, results, report_path)

    report_md_path = RESULTS_DIR / f"report_{run_id}.md"
    generate_report_md(meta, results, report_md_path)


if __name__ == "__main__":
    main()
